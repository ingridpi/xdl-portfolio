@article{Shen2020,
   abstract = {In the era of big data, deep learning for predicting stock market prices and trends has become even more popular than before. We collected 2 years of data from Chinese stock market and proposed a comprehensive customization of feature engineering and deep learning-based model for predicting price trend of stock markets. The proposed solution is comprehensive as it includes pre-processing of the stock market dataset, utilization of multiple feature engineering techniques, combined with a customized deep learning based system for stock market price trend prediction. We conducted comprehensive evaluations on frequently used machine learning models and conclude that our proposed solution outperforms due to the comprehensive feature engineering that we built. The system achieves overall high accuracy for stock market trend prediction. With the detailed design and evaluation of prediction term lengths, feature engineering, and data pre-processing methods, this work contributes to the stock analysis research community both in the financial and technical domains.},
   author = {Jingyi Shen and M. Omair Shafiq},
   doi = {10.1186/S40537-020-00333-6/TABLES/9},
   issn = {21961115},
   issue = {1},
   journal = {Journal of Big Data},
   keywords = {Deep learning,Feature engineering,Prediction,Stock market trend},
   month = {12},
   pages = {1-33},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Short-term stock market price trend prediction using a comprehensive deep learning system},
   volume = {7},
   url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00333-6},
   year = {2020}
}
@article{Lei2020,
   abstract = {Algorithmic trading is a continuous perception and decision making problem, where environment perception requires to learn feature representation from highly non-stationary and noisy financial time series, and decision making requires the algorithm to explore the environment and simultaneously make correct decisions in an online manner without any supervised information. To address these two problems, we propose a time-driven feature-aware jointly deep reinforcement learning model (TFJ-DRL) that integrates deep learning model and reinforcement learning model to improve the financial signal representation learning and action decision making in algorithmic trading. Concretely, we learn the environmental representation by adaptively selecting and re-weighting various features of financial signals and summarize the attention values between historical information and changing trend depending on the current state. Besides, the supervised deep learning and reinforcement learning are jointly and iteratively trained to make full use of the supervised signals in the training data, and obtain more update information and stricter loss function constraints, thereby increasing investment returns. TFJ-DRL is evaluated on real-world financial data with different price trends (rising, falling and no obvious direction). A series of analysis show the robust superiority and the extensive applicability of the proposed method.},
   author = {Kai Lei and Bing Zhang and Yu Li and Min Yang and Ying Shen},
   doi = {10.1016/J.ESWA.2019.112872},
   issn = {0957-4174},
   journal = {Expert Systems with Applications},
   keywords = {Algorithmic trading,Deep reinforcement learning,Gate,Temporal attention},
   month = {2},
   pages = {112872},
   publisher = {Pergamon},
   title = {Time-driven feature-aware jointly deep reinforcement learning for financial signal representation and algorithmic trading},
   volume = {140},
   year = {2020}
}
@article{Ma2021,
   abstract = {In recent years, deep reinforcement learning (DRL) algorithm has been widely used in algorithmic trading. Many fully automated trading systems or strategies have been built using DRL agents, which integrate price prediction and trading signal generation in one system. However, the previous agents extract the current state from the market data without considering the long-term market historical trend when making decisions. Besides, plenty of related and useful information has not been considered. To address these two problems, we propose a novel model named Parallel Multi-Module Deep Reinforcement Learning (PMMRL) algorithm. Here, two parallel modules are used to extract and encode the feature: one module employing Fully Connected (FC) layers is used to learn the current state from the market data of the traded stock and the fundamental data of the issuing company; another module using Long Short-Term Memory (LSTM) layers aims to detect the long-term historical trend of the market. The proposed model can extract features from the whole environment by the above two modules simultaneously, taking the advantages of both LSTM and FC layers. Extensive experiments on China stock market illustrate that the proposed PMMRL algorithm achieves a higher profit and a lower drawdown than several state-of-the-art algorithms.},
   author = {Cong Ma and Jiangshe Zhang and Junmin Liu and Lizhen Ji and Fei Gao},
   doi = {10.1016/J.NEUCOM.2021.04.005},
   issn = {0925-2312},
   journal = {Neurocomputing},
   keywords = {Capital asset pricing model,Long short-term memory,Parallel multi-module,Reinforcement learning},
   month = {8},
   pages = {290-302},
   publisher = {Elsevier},
   title = {A parallel multi-module deep reinforcement learning algorithm for stock trading},
   volume = {449},
   year = {2021}
}
@article{Hasan2024,
   abstract = {To efficiently capture diverse fluctuation profiles in forecasting crude oil prices, we here propose to combine heterogeneous predictors for forecasting the prices of crude oil. Specifically, a forecasting model is developed using blended ensemble learning that combines various machine learning methods, including k-nearest neighbour regression, regression trees, linear regression, ridge regression, and support vector regression. Data for Brent and WTI crude oil prices at various time series frequencies are used to validate the proposed blending ensemble learning approach. To show the validity of the proposed model, its performance is further benchmarked against existing individual and ensemble learning methods used for predicting crude oil price, such as lasso regression, bagging lasso regression, boosting, random forest, and support vector regression. We demonstrate that our proposed blending-based model dominates the existing forecasting models in terms of forecasting errors for both short- and medium-term horizons.},
   author = {Mahmudul Hasan and Mohammad Zoynul Abedin and Petr Hajek and Kristof Coussement and Md Nahid Sultan and Brian Lucey},
   doi = {10.1007/S10479-023-05810-8/TABLES/10},
   issn = {15729338},
   journal = {Annals of Operations Research},
   keywords = {Blending,Brent,Crude oil price,Ensemble learning,Forecasting,Stacking regression,WTI},
   month = {1},
   pages = {1-31},
   publisher = {Springer},
   title = {A blending ensemble learning model for crude oil price forecasting},
   url = {https://link.springer.com/article/10.1007/s10479-023-05810-8},
   year = {2024}
}
@article{Wu2023,
   abstract = {In today’s society, investment wealth management has become a mainstream of the contemporary era. Investment wealth management refers to the use of funds by investors to arrange funds reasonably, for example, savings, bank financial products, bonds, stocks, commodity spots, real estate, gold, art, and many others. Wealth management tools manage and assign families, individuals, enterprises, and institutions to achieve the purpose of increasing and maintaining value to accelerate asset growth. Among them, in investment and financial management, people’s favorite product of investment often stocks, because the stock market has great advantages and charm, especially compared with other investment methods. More and more scholars have developed methods of prediction from multiple angles for the stock market. According to the feature of financial time series and the task of price prediction, this article proposes a new framework structure to achieve a more accurate prediction of the stock price, which combines Convolution Neural Network (CNN) and Long–Short-Term Memory Neural Network (LSTM). This new method is aptly named stock sequence array convolutional LSTM (SACLSTM). It constructs a sequence array of historical data and its leading indicators (options and futures), and uses the array as the input image of the CNN framework, and extracts certain feature vectors through the convolutional layer and the layer of pooling, and as the input vector of LSTM, and takes ten stocks in U.S.A and Taiwan as the experimental data. Compared with previous methods, the prediction performance of the proposed algorithm in this article leads to better results when compared directly.},
   author = {Jimmy Ming Tai Wu and Zhongcui Li and Norbert Herencsar and Bay Vo and Jerry Chun Wei Lin},
   doi = {10.1007/S00530-021-00758-W/TABLES/15},
   issn = {14321882},
   issue = {3},
   journal = {Multimedia Systems},
   keywords = {Convolution neural network,Leading indicators,Long–short-term memory neural network,Stock price prediction},
   month = {6},
   pages = {1751-1770},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {A graph-based CNN-LSTM stock price prediction algorithm with leading indicators},
   volume = {29},
   url = {https://link.springer.com/article/10.1007/s00530-021-00758-w},
   year = {2023}
}
@article{Nti2020,
   abstract = {Stock-market prediction using machine-learning technique aims at developing effective and efficient models that can provide a better and higher rate of prediction accuracy. Numerous ensemble regressors and classifiers have been applied in stock market predictions, using different combination techniques. However, three precarious issues come in mind when constructing ensemble classifiers and regressors. The first concerns with the choice of base regressor or classifier technique adopted. The second concerns the combination techniques used to assemble multiple regressors or classifiers and the third concerns with the quantum of regressors or classifiers to be ensembled. Subsequently, the number of relevant studies scrutinising these previously mentioned concerns are limited. In this study, we performed an extensive comparative analysis of ensemble techniques such as boosting, bagging, blending and super learners (stacking). Using Decision Trees (DT), Support Vector Machine (SVM) and Neural Network (NN), we constructed twenty-five (25) different ensembled regressors and classifiers. We compared their execution times, accuracy, and error metrics over stock-data from Ghana Stock Exchange (GSE), Johannesburg Stock Exchange (JSE), Bombay Stock Exchange (BSE-SENSEX) and New York Stock Exchange (NYSE), from January 2012 to December 2018. The study outcome shows that stacking and blending ensemble techniques offer higher prediction accuracies (90–100%) and (85.7–100%) respectively, compared with that of bagging (53–97.78%) and boosting (52.7–96.32%). Furthermore, the root means square error (RMSE) recorded by stacking (0.0001–0.001) and blending (0.002–0.01) shows a better fit of ensemble classifiers and regressors based on these two techniques in market analyses compared with bagging (0.01–0.11) and boosting (0.01–0.443). Finally, the results undoubtedly suggest that an innovative study in the domain of stock market direction prediction ought to include ensemble techniques in their sets of algorithms.},
   author = {Isaac Kofi Nti and Adebayo Felix Adekoya and Benjamin Asubam Weyori},
   doi = {10.1186/S40537-020-00299-5/TABLES/25},
   issn = {21961115},
   issue = {1},
   journal = {Journal of Big Data},
   keywords = {Artificial intelligence,Blending,Ensemble-classifiers,Ensemble-regressors,Machine-learning,Predictions,Stacking},
   month = {12},
   pages = {1-40},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {A comprehensive evaluation of ensemble learning for stock-market prediction},
   volume = {7},
   url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00299-5},
   year = {2020}
}
@article{Guan2021,
   abstract = {Deep reinforcement learning (DRL) has been widely studied in the portfolio management task. However, it is challenging to understand a DRL-based trading strategy because of the black-box nature of deep neural networks. In this paper, we propose an empirical approach to explain the strategies of DRL agents for the portfolio management task. First, we use a linear model in hindsight as the reference model, which finds the best portfolio weights by assuming knowing actual stock returns in foresight. In particular, we use the coefficients of a linear model in hindsight as the reference feature weights. Secondly, for DRL agents, we use integrated gradients to define the feature weights, which are the coefficients between reward and features under a linear regression model. Thirdly, we study the prediction power in two cases, single-step prediction and multi-step prediction. In particular, we quantify the prediction power by calculating the linear correlations between the feature weights of a DRL agent and the reference feature weights, and similarly for machine learning methods. Finally, we evaluate a portfolio management task on Dow Jones 30 constituent stocks during 01/01/2009 to 09/01/2021. Our approach empirically reveals that a DRL agent exhibits a stronger multi-step prediction power than machine learning methods.},
   author = {Mao Guan and Xiao Yang Liu},
   doi = {10.1145/3490354.3494415},
   isbn = {9781450391481},
   journal = {ICAIF 2021 - 2nd ACM International Conference on AI in Finance},
   keywords = {Explainable deep reinforcement learning,Integrated Gradient,Reinforcement learning,Value iteration KEYWORDS Explainable deep reinforcement learning, Integrated Gradient, lin-ear model in hindsight, portfolio management * Equal contribution † Corresponding author,linear model in hindsight,portfolio management},
   month = {11},
   publisher = {Association for Computing Machinery, Inc},
   title = {Explainable Deep Reinforcement Learning for Portfolio Management: An Empirical Approach},
   url = {https://arxiv.org/abs/2111.03995v2},
   year = {2021}
}
@article{Liu2018,
   abstract = {Stock trading strategy plays a crucial role in investment companies. However, it is challenging to obtain optimal strategy in the complex and dynamic stock market. We explore the potential of deep reinforcement learning to optimize stock trading strategy and thus maximize investment return. 30 stocks are selected as our trading stocks and their daily prices are used as the training and trading market environment. We train a deep reinforcement learning agent and obtain an adaptive trading strategy. The agent's performance is evaluated and compared with Dow Jones Industrial Average and the traditional min-variance portfolio allocation strategy. The proposed deep reinforcement learning approach is shown to outperform the two baselines in terms of both the Sharpe ratio and cumulative returns.},
   author = {Xiao-Yang Liu and Zhuoran Xiong and Shan Zhong and Hongyang Yang and Anwar Walid},
   isbn = {1811.07522v3},
   journal = {NeurIPS 2018 AI in Finance Workshop.},
   month = {11},
   title = {Practical Deep Reinforcement Learning Approach for Stock Trading},
   url = {https://arxiv.org/abs/1811.07522v3},
   year = {2018}
}
@article{Yang2020,
   abstract = {Stock trading strategies play a critical role in investment. However, it is challenging to design a profitable strategy in a complex and dynamic stock market. In this paper, we propose an ensemble strategy that employs deep reinforcement schemes to learn a stock trading strategy by maximizing investment return. We train a deep reinforcement learning agent and obtain an ensemble trading strategy using three actor-critic based algorithms: Proximal Policy Optimization (PPO), Advantage Actor Critic (A2C), and Deep Deterministic Policy Gradient (DDPG). The ensemble strategy inherits and integrates the best features of the three algorithms, thereby robustly adjusting to different market situations. In order to avoid the large memory consumption in training networks with continuous action space, we employ a load-on-demand technique for processing very large data. We test our algorithms on the 30 Dow Jones stocks that have adequate liquidity. The performance of the trading agent with different reinforcement learning algorithms is evaluated and compared with both the Dow Jones Industrial Average index and the traditional min-variance portfolio allocation strategy. The proposed deep ensemble strategy is shown to outperform the three individual algorithms and two baselines in terms of the risk-adjusted return measured by the Sharpe ratio.},
   author = {Hongyang Yang and Xiao Yang Liu and Shan Zhong and Anwar Walid},
   doi = {10.1145/3383455.3422540},
   isbn = {9781450375849},
   journal = {ICAIF 2020 - 1st ACM International Conference on AI in Finance},
   keywords = {Actor-critic framework,Automated stock trading,Deep reinforcement learning,Ensemble strategy,Markov decision process},
   month = {10},
   publisher = {Association for Computing Machinery, Inc},
   title = {Deep reinforcement learning for automated stock trading: An ensemble strategy},
   url = {https://dl.acm.org/doi/10.1145/3383455.3422540},
   year = {2020}
}
@article{Moody2001,
   abstract = {We present methods for optimizing portfolios, asset allocations, and trading systems based on direct reinforcement (DR). In this approach, investment decision making is viewed as a stochastic control problem, and strategies are discovered directly. We present an adaptive algorithm called recurrent reinforcement learning (RRL) for discovering investment policies. The need to build forecasting models is eliminated, and better trading performance is obtained. The direct reinforcement approach differs from dynamic programming and reinforcement algorithms such as TD-learning and Q-learning, which attempt to estimate a value function for the control problem. We find that the RRL direct reinforcement framework enables a simpler problem representation, avoids Bellman's curse of dimensionality and offers compelling advantages in efficiency. We demonstrate how direct reinforcement can be used to optimize risk-adjusted investment returns (including the differential Sharpe ratio), while accounting for the effects of transaction costs. In extensive simulation work using real financial data, we find that our approach based on RRL produces better trading strategies than systems utilizing Q-Learning (a value function method). Real-world applications include an intra-daily currency trader and a monthly asset allocation system for the S&P 500 Stock Index and T-Bills.},
   author = {John Moody and Matthew Saffell},
   doi = {10.1109/72.935097},
   issn = {10459227},
   issue = {4},
   journal = {IEEE Transactions on Neural Networks},
   keywords = {Differential Sharpe ratio,Direct reinforcement (DR),Downside deviation,Policy gradient,Q-learning,Recurrent reinforcement learning,Risk,TD-learning,Trading,Value function},
   month = {7},
   pages = {875-889},
   title = {Learning to trade via direct reinforcement},
   volume = {12},
   year = {2001}
}
@article{Cortes2024,
   abstract = {While machine learning's role in financial trading has advanced considerably, algorithmic transparency and explainability challenges still exist. This research enriches prior studies focused on high-frequency financial data prediction by introducing an explainable reinforcement learning model for portfolio management. This model transcends basic asset prediction, formulating concrete, actionable trading strategies. The methodology is applied in a custom trading environment mimicking the CAC-40 index's financial conditions, allowing the model to adapt dynamically to market changes based on iterative learning from historical data. Empirical findings reveal that the model outperforms an equally weighted portfolio in out-of-sample tests. The study offers a dual contribution: it elevates algorithmic planning while significantly boosting transparency and interpretability in financial machine learning. This approach tackles the enduring ‘black-box’ issue and provides a holistic, transparent framework for managing investment portfolios.},
   author = {Daniel González Cortés and Enrique Onieva and Iker Pastor and Laura Trinchera and Jian Wu},
   doi = {10.1111/EXSY.13667},
   issn = {1468-0394},
   issue = {11},
   journal = {Expert Systems},
   keywords = {algorithmic transparency,explainable reinforcement learning,finance,portfolio management},
   month = {11},
   pages = {e13667},
   publisher = {John Wiley \& Sons, Ltd},
   title = {Portfolio construction using explainable reinforcement learning},
   volume = {41},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1111/exsy.13667 https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.13667 https://onlinelibrary.wiley.com/doi/10.1111/exsy.13667},
   year = {2024}
}
@article{BarredoArrieta2019,
   abstract = {In the last years, Artificial Intelligence (AI) has achieved a notable momentum that may deliver the best of expectations over many application sectors across the field. For this to occur, the entire community stands in front of the barrier of explainability, an inherent problem of AI techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI. Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is acknowledged as a crucial feature for the practical deployment of AI models. This overview examines the existing literature in the field of XAI, including a prospect toward what is yet to be reached. We summarize previous efforts to define explainability in Machine Learning, establishing a novel definition that covers prior conceptual propositions with a major focus on the audience for which explainability is sought. We then propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at Deep Learning methods for which a second taxonomy is built. This literature analysis serves as the background for a series of challenges faced by XAI, such as the crossroads between data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to XAI with a reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
   author = {Alejandro Barredo Arrieta and Natalia Díaz-Rodríguez and Javier Del Ser and Adrien Bennetot and Siham Tabik and Alberto Barbado and Salvador Garcia and Sergio Gil-Lopez and Daniel Molina and Richard Benjamins and Raja Chatila and Francisco Herrera},
   doi = {10.1016/j.inffus.2019.12.012},
   issn = {15662535},
   journal = {Information Fusion},
   keywords = {Accountability,Comprehensibility,Data Fusion,Deep Learning,Explainable Artificial Intelligence,Fairness,Interpretability,Machine Learning,Privacy,Responsible Artificial Intelligence,Transparency},
   month = {10},
   pages = {82-115},
   publisher = {Elsevier B.V.},
   title = {Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI},
   volume = {58},
   url = {https://arxiv.org/abs/1910.10045v2},
   year = {2019}
}
@article{Gunning2019,
   abstract = {Explainability is essential for users to effectively understand, trust, and manage powerful artificial intelligence applications.},
   author = {David Gunning and Mark Stefik and Jaesik Choi and Timothy Miller and Simone Stumpf and Guang Zhong Yang},
   doi = {10.1126/SCIROBOTICS.AAY7120},
   issn = {24709476},
   issue = {37},
   journal = {Science Robotics},
   month = {12},
   pmid = {33137719},
   publisher = {American Association for the Advancement of Science},
   title = {XAI—Explainable artificial intelligence},
   volume = {4},
   url = {https://www.science.org/doi/10.1126/scirobotics.aay7120},
   year = {2019}
}
@article{Brigo2021,
   abstract = {Deep learning is a powerful tool whose applications in quantitative finance are growing every day. Yet, artificial neural networks behave as black boxes and this hinders validation and accountability processes. Being able to interpret the inner functioning and the input-output relationship of these networks has become key for the acceptance of such tools. In this paper we focus on the calibration process of a stochastic volatility model, a subject recently tackled by deep learning algorithms. We analyze the Heston model in particular, as this model's properties are well known, resulting in an ideal benchmark case. We investigate the capability of local strategies and global strategies coming from cooperative game theory to explain the trained neural networks, and we find that global strategies such as Shapley values can be effectively used in practice. Our analysis also highlights that Shapley values may help choose the network architecture, as we find that fully-connected neural networks perform better than convolutional neural networks in predicting and interpreting the Heston model prices to parameters relationship.},
   author = {Damiano Brigo and Xiaoshan Huang and Andrea Pallavicini and Haitz Saez de Ocariz Borde},
   doi = {10.2139/ssrn.3829947},
   journal = {SSRN Electronic Journal},
   keywords = {91G20,91G60 Keywords: Volatility Smile,AMS classification codes: 68T07,Deep Learning,Heston model,Interpretability Models,Option Pricing,Shapley Values,Smile parameters,Stochastic Volatility,Surrogate Models},
   month = {4},
   publisher = {Elsevier BV},
   title = {Interpretability in deep learning for finance: a case study for the Heston model},
   url = {https://arxiv.org/abs/2104.09476v1},
   year = {2021}
}
@article{GarciaCespedes2025,
   abstract = {Machine Learning models explainability has recently become a very popular topic in the banking sector. We apply Shapley values and the Python library SHAP to a real credit risk database and show that the two available SHAP types (interventional and path-dependent) provide very similar results even under correlated features. The main drawback of SHAP is that it is not portfolio invariant, this is, the explanation for the prediction provided for each observation depends on the portfolio distribution of the features. This can be a serious problem for customers and banking regulators, who expect that explanations will stay stable as long as the clients characteristics do not change. We conduct several tests and show that the SHAP explanation of an observation may considerably change depending on the rest of the portfolio distribution. As a consequence, the explanation of a client may vary over time even if her characteristics do not change and banks using the same model (for ex. commercial models) may provide different explanations to the same client.},
   author = {Rubén García-Céspedes and Francisco J. Alias-Carrascosa and Manuel Moreno},
   doi = {10.1080/01605682.2025.2485263},
   issn = {14769360},
   journal = {Journal of the Operational Research Society},
   keywords = {Finance,Machine Learning,SHAP,credit risk},
   publisher = {Taylor and Francis Ltd.},
   title = {On Machine Learning models explainability in the banking sector: the case of SHAP},
   year = {2025}
}
@inbook{Bruce2014,
   abstract = {Once a security is selected, its weight or position must be determined in order to construct the portfolio. Portfolio construction is the process of using security characteristics to combine the securities in a way that optimizes the portfolio’s outcomes. Security selection is usually considered the primary driver of portfolio returns, and portfolio construction is usually considered to primarily impact the portfolio’s risk. This chapter discusses the how characteristics of portfolio securities combine to determine the portfolio’s absolute risk and risk relative to a benchmark. The chapter also offers practical considerations to student-managed investment funds in optimizing a portfolio for various objectives.},
   author = {Brian Bruce and Jason Greene},
   city = {San Diego},
   doi = {https://doi.org/10.1016/B978-0-12-374755-6.00004-2},
   isbn = {978-0-12-374755-6},
   booktitle = {Trading and Money Management in a Student-Managed Portfolio},
   keywords = {Information Ratio,Sharpe Ratio,active weights,correlation,correlation matrix,covariance,covariance matrix,expected return,optimization,parameter uncertainty,portfolio construction,relative return,relative risk,security selection,standard deviation,total risk,tracking error,variance,weights},
   pages = {133-178},
   publisher = {Academic Press},
   title = {Chapter 4 - Portfolio Construction},
   url = {https://www.sciencedirect.com/science/article/pii/B9780123747556000042},
   year = {2014}
}
@article{Li2019,
   abstract = {Portfolio allocation is crucial for investment companies. However, getting the best strategy in a complex and dynamic stock market is challenging. In this paper, we propose a novel Adaptive Deep Deterministic Reinforcement Learning scheme (Adaptive DDPG) for the portfolio allocation task, which incorporates optimistic or pessimistic deep reinforcement learning that is reflected in the influence from prediction errors. Dow Jones 30 component stocks are selected as our trading stocks and their daily prices are used as the training and testing data. We train the Adaptive DDPG agent and obtain a trading strategy. The Adaptive DDPG's performance is compared with the vanilla DDPG, Dow Jones Industrial Average index and the traditional min-variance and mean-variance portfolio allocation strategies. Adaptive DDPG outperforms the baselines in terms of the investment return and the Sharpe ratio.},
   author = {Xinyi Li and Yinchuan Li and Yuancheng Zhan and Xiao-Yang Liu},
   month = {6},
   title = {Optimistic Bull or Pessimistic Bear: Adaptive Deep Reinforcement Learning for Stock Portfolio Allocation},
   url = {https://arxiv.org/abs/1907.01503v1},
   year = {2019},
   journal = {arXiv preprint arXiv:1907.01503}
}
@article{Sato2019,
   abstract = {Financial portfolio management is one of the problems that are most frequently encountered in the investment industry. Nevertheless, it is not widely recognized that both Kelly Criterion and Risk Parity collapse into Mean Variance under some conditions, which implies that a universal solution to the portfolio optimization problem could potentially exist. In fact, the process of sequential computation of optimal component weights that maximize the portfolio's expected return subject to a certain risk budget can be reformulated as a discrete-time Markov Decision Process (MDP) and hence as a stochastic optimal control, where the system being controlled is a portfolio consisting of multiple investment components, and the control is its component weights. Consequently, the problem could be solved using model-free Reinforcement Learning (RL) without knowing specific component dynamics. By examining existing methods of both value-based and policy-based model-free RL for the portfolio optimization problem, we identify some of the key unresolved questions and difficulties facing today's portfolio managers of applying model-free RL to their investment portfolios.},
   author = {Yoshiharu Sato},
   keywords = {Portfolio Management,Quantitative Finance,Reinforcement Learning},
   month = {4},
   title = {Model-Free Reinforcement Learning for Financial Portfolios: A Brief Survey},
   url = {https://arxiv.org/abs/1904.04973v2},
   journal={arXiv preprint arXiv:1904.04973},
   year = {2019}
}
@misc{kent,
   author = {Financial Mathematics Clinic},
   editor = {University of Kent},
   pages = {8},
   title = {Mean Variance Portfolio Theory},
   url = {https://www.kent.ac.uk/learning/documents/slas-documents/mean-variance-portfolio.pdf}
}
@article{Markowitz1952,
   author = {Harry Markowitz},
   doi = {10.2307/2975974},
   isbn = {9789812833655},
   issn = {00221082},
   issue = {1},
   journal = {The Journal of Finance},
   month = {3},
   pages = {77-91},
   publisher = {World Scientific Publishing Co.},
   title = {Portfolio selection},
   volume = {7},
   url = {https://www.jstor.org/stable/2975974},
   year = {1952}
}
@book{Bodie2014,
   author = {Zvi. Bodie and Alex. Kane and Alan J.. Marcus},
   edition = {10},
   isbn = {9780077861674},
   pages = {220-221},
   publisher = {McGraw-Hill Education},
   title = {Investments},
   year = {2014}
}
@article{Tang2024,
   abstract = {Reinforcement learning (RL), particularly its combination with deep neural networks referred to as deep RL (DRL), has shown tremendous promise across a wide range of applications, suggesting its potential for enabling the development of sophisticated robotic behaviors. Robotics problems, however, pose fundamental difficulties for the application of RL, stemming from the complexity and cost of interacting with the physical world. This article provides a modern survey of DRL for robotics, with a particular focus on evaluating the real-world successes achieved with DRL in realizing several key robotic competencies. Our analysis aims to identify the key factors underlying those exciting successes, reveal underexplored areas, and provide an overall characterization of the status of DRL in robotics. We highlight several important avenues for future work, emphasizing the need for stable and sample-efficient real-world RL paradigms, holistic approaches for discovering and integrating various competencies to tackle complex long-horizon, open-world tasks, and principled development and evaluation procedures. This survey is designed to offer insights for both RL practitioners and roboticists toward harnessing RL's power to create generally capable real-world robotic systems.},
   author = {Chen Tang and Ben Abbatematteo and Jiaheng Hu and Rohan Chandra and Roberto Martín-Martín and Peter Stone},
   doi = {10.1146/annurev-control-030323-022510},
   issn = {25735144},
   issue = {1},
   journal = {Annual Review of Control, Robotics, and Autonomous Systems},
   keywords = {deep learning,learning for control,real-world applications,reinforcement learning,robotics},
   month = {8},
   pages = {153-188},
   publisher = {Annual Reviews Inc.},
   title = {Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes},
   volume = {8},
   url = {https://arxiv.org/abs/2408.03539v3},
   year = {2024}
}
@article{Shao2019,
   abstract = {Deep reinforcement learning (DRL) has made great achievements since proposed. Generally, DRL agents receive high-dimensional inputs at each step, and make actions according to deep-neural-network-based policies. This learning mechanism updates the policy to maximize the return with an end-to-end method. In this paper, we survey the progress of DRL methods, including value-based, policy gradient, and model-based algorithms, and compare their main techniques and properties. Besides, DRL plays an important role in game artificial intelligence (AI). We also take a review of the achievements of DRL in various video games, including classical Arcade games, first-person perspective games and multi-agent real-time strategy games, from 2D to 3D, and from single-agent to multi-agent. A large number of video game AIs with DRL have achieved super-human performance, while there are still some challenges in this domain. Therefore, we also discuss some key points when applying DRL methods to this field, including exploration-exploitation, sample efficiency, generalization and transfer, multi-agent learning, imperfect information, and delayed spare rewards, as well as some research directions.},
   author = {Kun Shao and Zhentao Tang and Yuanheng Zhu and Nannan Li and Dongbin Zhao},
   keywords = {Index Terms-reinforcement learning,deep learning,deep reinforcement learning,game AI,video games},
   month = {12},
   title = {A Survey of Deep Reinforcement Learning in Video Games},
   url = {https://arxiv.org/abs/1912.10944v2},
   year = {2019}
}
@article{Silver2016,
   abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away. A computer Go program based on deep neural networks defeats a human professional player to achieve one of the grand challenges of artificial intelligence. The victory in 1997 of the chess-playing computer Deep Blue in a six-game series against the then world champion Gary Kasparov was seen as a significant milestone in the development of artificial intelligence. An even greater challenge remained — the ancient game of Go. Despite decades of refinement, until recently the strongest computers were still playing Go at the level of human amateurs. Enter AlphaGo. Developed by Google DeepMind, this program uses deep neural networks to mimic expert players, and further improves its performance by learning from games played against itself. AlphaGo has achieved a 99% win rate against the strongest other Go programs, and defeated the reigning European champion Fan Hui 5–0 in a tournament match. This is the first time that a computer program has defeated a human professional player in even games, on a full, 19 x 19 board, in even games with no handicap.},
   author = {David Silver and Aja Huang and Chris J. Maddison and Arthur Guez and Laurent Sifre and George Van Den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Veda Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and Timothy Lillicrap and Madeleine Leach and Koray Kavukcuoglu and Thore Graepel and Demis Hassabis},
   doi = {10.1038/nature16961},
   issn = {1476-4687},
   issue = {7587},
   journal = {Nature 2016 529:7587},
   keywords = {Computational science,Computer science,Reward},
   month = {1},
   pages = {484-489},
   pmid = {26819042},
   publisher = {Nature Publishing Group},
   title = {Mastering the game of Go with deep neural networks and tree search},
   volume = {529},
   url = {https://www.nature.com/articles/nature16961},
   year = {2016}
}
@article{Francois-Lavet2018,
   abstract = {Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.},
   author = {Vincent François-Lavet and Peter Henderson and Riashat Islam and Marc G. Bellemare and Joelle Pineau},
   doi = {10.1561/2200000071},
   issue = {3-4},
   journal = {Foundations and Trends in Machine Learning},
   month = {12},
   pages = {219-354},
   publisher = {Now Publishers Inc},
   title = {An Introduction to Deep Reinforcement Learning},
   volume = {11},
   url = {http://arxiv.org/abs/1811.12560 http://dx.doi.org/10.1561/2200000071},
   year = {2018}
}
@book{Goodfellow2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
@misc{IBM2021,
   author = {IBM},
   month = {9},
   title = {What Is Machine Learning (ML)?},
   url = {https://www.ibm.com/think/topics/machine-learning},
   year = {2021}
}
@article{Bellman1957,
   author = {Richard Bellman},
   issue = {5},
   journal = {Journal of Mathematics and Mechanics},
   pages = {679-684},
   title = {A Markovian Decision Process},
   volume = {6},
   url = {https://www.jstor.org/stable/24900506},
   year = {1957}
}

@book{Bellman1957book,
   abstract = {This classic book is an introduction to dynamic programming, presented by the scientist who coined the term and developed the theory in its early stages. In Dynamic Programming, Richard E. Bellman introduces his groundbreaking theory and furnishes a new and versatile mathematical tool for the treatment of many complex problems, both within and outside of the discipline. The book is written at a moderate mathematical level, requiring only a basic foundation in mathematics, including calculus. The applications formulated and analyzed in such diverse fields as mathematical economics, logistics, scheduling theory, communication theory, and control processes are as relevant today as they were when Bellman first presented them. A new introduction by Stuart Dreyfus reviews Bellman’s later work on dynamic programming and identifies important research areas that have profited from the application of Bellman’s theory.},
   author = {Richard Bellman},
   doi = {10.2307/j.ctv1nxcw0f},
   isbn = {9781400835386},
   journal = {Dynamic Programming},
   month = {1},
   pages = {1-346},
   publisher = {Princeton University Press},
   title = {Dynamic Programming},
   url = {https://press.princeton.edu/books/ebook/9781400835386/dynamic-programming-pdf},
   year = {1957}
}
@techReport{Thrun1992,
   author = {Sebastian Thrun},
   city = {Pittsburgh, PA},
   institution = {Carnegie Mellon University},
   month = {1},
   title = {Efficient Exploration In Reinforcement Learning},
   url = {https://www.ri.cmu.edu/publications/efficient-exploration-in-reinforcement-learning/},
   year = {1992}
}
