@misc{Achiam2018,
  author = {Josh Achiam},
  title  = {Part 2: Kinds of RL Algorithms},
  url    = {https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html},
  year   = {2018}
}
@article{AIAct2024,
  author  = {European Parliament and Council of the European Union},
  journal = {Publications Office of the European Union},
  month   = {6},
  title   = {Regulation (EU) 2024/1689},
  url     = {https://eur-lex.europa.eu/legal-content/EN/LSU/?uri=CELEX:32024R1689},
  year    = {2024}
}
@article{Amirshahi2023,
  abstract  = {In recent years, cryptocurrencies' price prediction has attracted the interest of many people including investors, researchers and practitioners. In this study, we proposed a hybrid model for predicting the daily close price of cryptocurrencies based on different neural networks such as long short-term memory, convolutional neural network and attention mechanism. Using an ensemble of three pre-trained language models, we extracted sentiment of cryptocurrency-related tweets posted between 1 January 2021 and 31 December 2021. We constructed 20 different versions of our model and evaluated their performance on data of 27 most traded cryptocurrencies using a history of previous days' sentiment data along with close prices as input data. The flexible input layer of our model enables different ways of feeding data into the model to adjust it for different cryptocurrencies to obtain better predictions. Our analysis revealed several important findings. We showed that longer sequences of input data achieve most accurate predictions on average. More specifically, using a history of 14- and 21-days' data results in lowest RMSE values on average compared to using a history of 7 days. However, there is no significant difference between the results related to the input sequences with lengths of 14 and 21. In addition, our findings suggest that sentiment data can be useful in predicting prices for more than 70% of the studied cryptocurrencies. Thus, peoples' emotions, opinions, and sentiment that are expressed through their posts on Twitter platform play a significant role in prediction of cryptocurrencies' prices.},
  author    = {Bahareh Amirshahi and Salim Lahmiri},
  doi       = {10.1111/EXSY.13428},
  issn      = {1468-0394},
  issue     = {1},
  journal   = {Expert Systems},
  keywords  = {cryptocurrencies,deep learning,hybrid model,price prediction,sentiment analysis},
  month     = {8},
  pages     = {e13428},
  publisher = {John Wiley and Sons Inc},
  title     = {Investigating the effectiveness of Twitter sentiment in cryptocurrency close price prediction by using deep learning},
  volume    = {42},
  url       = {https://onlinelibrary.wiley.com/doi/full/10.1111/exsy.13428 https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.13428 https://onlinelibrary.wiley.com/doi/10.1111/exsy.13428},
  year      = {2023}
}
@article{BarredoArrieta2019,
  abstract  = {In the last years, Artificial Intelligence (AI) has achieved a notable momentum that may deliver the best of expectations over many application sectors across the field. For this to occur, the entire community stands in front of the barrier of explainability, an inherent problem of AI techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI. Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is acknowledged as a crucial feature for the practical deployment of AI models. This overview examines the existing literature in the field of XAI, including a prospect toward what is yet to be reached. We summarize previous efforts to define explainability in Machine Learning, establishing a novel definition that covers prior conceptual propositions with a major focus on the audience for which explainability is sought. We then propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at Deep Learning methods for which a second taxonomy is built. This literature analysis serves as the background for a series of challenges faced by XAI, such as the crossroads between data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to XAI with a reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
  author    = {Alejandro Barredo Arrieta and Natalia Díaz-Rodríguez and Javier Del Ser and Adrien Bennetot and Siham Tabik and Alberto Barbado and Salvador Garcia and Sergio Gil-Lopez and Daniel Molina and Richard Benjamins and Raja Chatila and Francisco Herrera},
  doi       = {10.1016/j.inffus.2019.12.012},
  issn      = {15662535},
  journal   = {Information Fusion},
  keywords  = {Accountability,Comprehensibility,Data Fusion,Deep Learning,Explainable Artificial Intelligence,Fairness,Interpretability,Machine Learning,Privacy,Responsible Artificial Intelligence,Transparency},
  month     = {10},
  pages     = {82-115},
  publisher = {Elsevier B.V.},
  title     = {Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI},
  volume    = {58},
  url       = {https://arxiv.org/abs/1910.10045v2},
  year      = {2019}
}
@article{Bellemare2017,
  abstract  = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
  author    = {Marc G. Bellemare and Will Dabney and Rémi Munos},
  isbn      = {9781510855144},
  journal   = {34th International Conference on Machine Learning, ICML 2017},
  month     = {7},
  pages     = {693-711},
  publisher = {International Machine Learning Society (IMLS)},
  title     = {A Distributional Perspective on Reinforcement Learning},
  volume    = {1},
  url       = {https://arxiv.org/abs/1707.06887v1},
  year      = {2017}
}
@article{Bellman1957,
  author  = {Richard Bellman},
  issue   = {5},
  journal = {Journal of Mathematics and Mechanics},
  pages   = {679-684},
  title   = {A Markovian Decision Process},
  volume  = {6},
  url     = {https://www.jstor.org/stable/24900506},
  year    = {1957}
}
@book{Bellman1957book,
  abstract  = {This classic book is an introduction to dynamic programming, presented by the scientist who coined the term and developed the theory in its early stages. In Dynamic Programming, Richard E. Bellman introduces his groundbreaking theory and furnishes a new and versatile mathematical tool for the treatment of many complex problems, both within and outside of the discipline. The book is written at a moderate mathematical level, requiring only a basic foundation in mathematics, including calculus. The applications formulated and analyzed in such diverse fields as mathematical economics, logistics, scheduling theory, communication theory, and control processes are as relevant today as they were when Bellman first presented them. A new introduction by Stuart Dreyfus reviews Bellman’s later work on dynamic programming and identifies important research areas that have profited from the application of Bellman’s theory.},
  author    = {Richard Bellman},
  doi       = {10.2307/j.ctv1nxcw0f},
  isbn      = {9781400835386},
  journal   = {Dynamic Programming},
  month     = {1},
  pages     = {1-346},
  publisher = {Princeton University Press},
  title     = {Dynamic Programming},
  url       = {https://press.princeton.edu/books/ebook/9781400835386/dynamic-programming-pdf},
  year      = {1957}
}
@book{Bodie2014,
  author    = {Zvi. Bodie and Alex. Kane and Alan J.. Marcus},
  edition   = {10},
  isbn      = {9780077861674},
  pages     = {220-221},
  publisher = {McGraw-Hill Education},
  title     = {Investments},
  year      = {2014}
}
@article{Breiman2001,
  abstract  = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, * * *, 148-156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  author    = {Leo Breiman},
  doi       = {10.1023/A:1010933404324/METRICS},
  issn      = {08856125},
  journal   = {Machine Learning},
  keywords  = {Classification,Ensemble,Regression},
  month     = {10},
  pages     = {5-32},
  publisher = {Springer},
  title     = {Random Forests},
  volume    = {45},
  url       = {https://link.springer.com/article/10.1023/A:1010933404324},
  year      = {2001}
}
@article{Brigo2021,
  abstract  = {Deep learning is a powerful tool whose applications in quantitative finance are growing every day. Yet, artificial neural networks behave as black boxes and this hinders validation and accountability processes. Being able to interpret the inner functioning and the input-output relationship of these networks has become key for the acceptance of such tools. In this paper we focus on the calibration process of a stochastic volatility model, a subject recently tackled by deep learning algorithms. We analyze the Heston model in particular, as this model's properties are well known, resulting in an ideal benchmark case. We investigate the capability of local strategies and global strategies coming from cooperative game theory to explain the trained neural networks, and we find that global strategies such as Shapley values can be effectively used in practice. Our analysis also highlights that Shapley values may help choose the network architecture, as we find that fully-connected neural networks perform better than convolutional neural networks in predicting and interpreting the Heston model prices to parameters relationship.},
  author    = {Damiano Brigo and Xiaoshan Huang and Andrea Pallavicini and Haitz Saez de Ocariz Borde},
  doi       = {10.2139/ssrn.3829947},
  journal   = {SSRN Electronic Journal},
  keywords  = {91G20,91G60 Keywords: Volatility Smile,AMS classification codes: 68T07,Deep Learning,Heston model,Interpretability Models,Option Pricing,Shapley Values,Smile parameters,Stochastic Volatility,Surrogate Models},
  month     = {4},
  publisher = {Elsevier BV},
  title     = {Interpretability in deep learning for finance: a case study for the Heston model},
  url       = {https://arxiv.org/abs/2104.09476v1},
  year      = {2021}
}
@inbook{Bruce2014,
  abstract  = {Once a security is selected, its weight or position must be determined in order to construct the portfolio. Portfolio construction is the process of using security characteristics to combine the securities in a way that optimizes the portfolio’s outcomes. Security selection is usually considered the primary driver of portfolio returns, and portfolio construction is usually considered to primarily impact the portfolio’s risk. This chapter discusses the how characteristics of portfolio securities combine to determine the portfolio’s absolute risk and risk relative to a benchmark. The chapter also offers practical considerations to student-managed investment funds in optimizing a portfolio for various objectives.},
  author    = {Brian Bruce and Jason Greene},
  city      = {San Diego},
  doi       = {https://doi.org/10.1016/B978-0-12-374755-6.00004-2},
  isbn      = {978-0-12-374755-6},
  booktitle = {Trading and Money Management in a Student-Managed Portfolio},
  keywords  = {Information Ratio,Sharpe Ratio,active weights,correlation,correlation matrix,covariance,covariance matrix,expected return,optimization,parameter uncertainty,portfolio construction,relative return,relative risk,security selection,standard deviation,total risk,tracking error,variance,weights},
  pages     = {133-178},
  publisher = {Academic Press},
  title     = {Chapter 4 - Portfolio Construction},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780123747556000042},
  year      = {2014}
}
@article{Chaudhary2025,
  abstract = {Predicting stock market movements remains a persistent challenge due to the inherently volatile, non-linear, and stochastic nature of financial time series data. This paper introduces a deep learning-based framework employing Long Short-Term Memory (LSTM) networks to forecast the closing stock prices of major technology firms: Apple, Google, Microsoft, and Amazon, listed on NASDAQ. Historical data was sourced from Yahoo Finance and processed using normalization and feature engineering techniques. The proposed model achieves a Mean Absolute Percentage Error (MAPE) of 2.72 on unseen test data, significantly outperforming traditional models like ARIMA. To further enhance predictive accuracy, sentiment scores were integrated using real-time news articles and social media data, analyzed through the VADER sentiment analysis tool. A web application was also developed to provide real-time visualizations of stock price forecasts, offering practical utility for both individual and institutional investors. This research demonstrates the strength of LSTM networks in modeling complex financial sequences and presents a novel hybrid approach combining time series modeling with sentiment analysis.},
  author   = {Rajneesh Chaudhary},
  isbn     = {2505.05325v1},
  keywords = {Deep Learning,Financial Modeling,Long Short-Term Memory (LSTM),Sentiment Analysis,Stock Market Prediction,Time Series Forecasting,Web Application},
  month    = {5},
  title    = {Advanced Stock Market Prediction Using Long Short-Term Memory Networks: A Comprehensive Deep Learning Framework},
  url      = {http://arxiv.org/abs/2505.05325},
  year     = {2025},
  journal  = {Journal of Financial Data Science}
}
@article{Cortes2024,
  abstract  = {While machine learning's role in financial trading has advanced considerably, algorithmic transparency and explainability challenges still exist. This research enriches prior studies focused on high-frequency financial data prediction by introducing an explainable reinforcement learning model for portfolio management. This model transcends basic asset prediction, formulating concrete, actionable trading strategies. The methodology is applied in a custom trading environment mimicking the CAC-40 index's financial conditions, allowing the model to adapt dynamically to market changes based on iterative learning from historical data. Empirical findings reveal that the model outperforms an equally weighted portfolio in out-of-sample tests. The study offers a dual contribution: it elevates algorithmic planning while significantly boosting transparency and interpretability in financial machine learning. This approach tackles the enduring ‘black-box’ issue and provides a holistic, transparent framework for managing investment portfolios.},
  author    = {Daniel González Cortés and Enrique Onieva and Iker Pastor and Laura Trinchera and Jian Wu},
  doi       = {10.1111/EXSY.13667},
  issn      = {1468-0394},
  issue     = {11},
  journal   = {Expert Systems},
  keywords  = {algorithmic transparency,explainable reinforcement learning,finance,portfolio management},
  month     = {11},
  pages     = {e13667},
  publisher = {John Wiley \& Sons, Ltd},
  title     = {Portfolio construction using explainable reinforcement learning},
  volume    = {41},
  url       = {https://onlinelibrary.wiley.com/doi/full/10.1111/exsy.13667 https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.13667 https://onlinelibrary.wiley.com/doi/10.1111/exsy.13667},
  year      = {2024}
}
@article{de-La-Rica-Escudero2025,
  abstract  = {Financial portfolio management investment policies computed quantitatively by modern portfolio theory techniques like the Markowitz model rely on a set of assumptions that are not supported by data in high volatility markets such as the technological sector or cryptocurrencies. Hence, quantitative researchers are looking for alternative models to tackle this problem. Concretely, portfolio management (PM) is a problem that has been successfully addressed recently by Deep Reinforcement Learning (DRL) approaches. In particular, DRL algorithms train an agent by estimating the distribution of the expected reward of every action performed by an agent given any financial state in a simulator, also called gymnasium. However, these methods rely on Deep Neural Networks model to represent such a distribution, that although they are universal approximator models, capable of representing this distribution over time, they cannot explain its behaviour, given by a set of parameters that are not interpretable. Critically, financial investors policies require predictions to be interpretable, to assess whether they follow a reasonable behaviour, so DRL agents are not suited to follow a particular policy or explain their actions. In this work, driven by the motivation of making DRL explainable, we developed a novel Explainable DRL (XDRL) approach for PM, integrating the Proximal Policy Optimization (PPO) DRL algorithm with the model agnostic explainable machine learning techniques of feature importance, SHAP and LIME to enhance transparency in prediction time. By executing our methodology, we can interpret in prediction time the actions of the agent to assess whether they follow the requisites of an investment policy or to assess the risk of following the agent’s suggestions. We empirically illustrate it by successfully identifying key features influencing investment decisions, which demonstrate the ability to explain the agent actions in prediction time. We propose the first explainable post hoc PM financial policy of a DRL agent.},
  author    = {Alejandra de-La-Rica-Escudero and Eduardo C. Garrido-Merchán and María Coronado-Vaca},
  doi       = {10.1371/JOURNAL.PONE.0315528},
  isbn      = {1111111111},
  issn      = {1932-6203},
  issue     = {1},
  journal   = {PLOS ONE},
  keywords  = {Algorithms,Decision making,Finance,Financial management,Financial markets,Monetary policy,Neural networks,Optimization},
  month     = {1},
  pages     = {e0315528},
  pmid      = {39820096},
  publisher = {Public Library of Science},
  title     = {Explainable post hoc portfolio management financial policy of a Deep Reinforcement Learning agent},
  volume    = {20},
  url       = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0315528},
  year      = {2025}
}
@article{Falkner2018,
  abstract  = {Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typically computationally infeasible. On the other hand, bandit-based configuration evaluation approaches based on random search lack guidance and do not converge to the best configurations as quickly. Here, we propose to combine the benefits of both Bayesian optimization and bandit-based methods, in order to achieve the best of both worlds: strong anytime performance and fast convergence to optimal configurations. We propose a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being conceptually simple and easy to implement.},
  author    = {Stefan Falkner and Aaron Klein and Frank Hutter},
  isbn      = {9781510867963},
  journal   = {35th International Conference on Machine Learning, ICML 2018},
  month     = {7},
  pages     = {2323-2341},
  publisher = {International Machine Learning Society (IMLS)},
  title     = {BOHB: Robust and Efficient Hyperparameter Optimization at Scale},
  volume    = {4},
  url       = {https://arxiv.org/abs/1807.01774v1},
  year      = {2018}
}
@article{Francois-Lavet2018,
  abstract  = {Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.},
  author    = {Vincent François-Lavet and Peter Henderson and Riashat Islam and Marc G. Bellemare and Joelle Pineau},
  doi       = {10.1561/2200000071},
  issue     = {3-4},
  journal   = {Foundations and Trends in Machine Learning},
  month     = {12},
  pages     = {219-354},
  publisher = {Now Publishers Inc},
  title     = {An Introduction to Deep Reinforcement Learning},
  volume    = {11},
  url       = {http://arxiv.org/abs/1811.12560 http://dx.doi.org/10.1561/2200000071},
  year      = {2018}
}
@article{Fujimoto2018,
  abstract  = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  author    = {Scott Fujimoto and Herke Van Hoof and David Meger},
  isbn      = {9781510867963},
  journal   = {35th International Conference on Machine Learning, ICML 2018},
  month     = {2},
  pages     = {2587-2601},
  publisher = {International Machine Learning Society (IMLS)},
  title     = {Addressing Function Approximation Error in Actor-Critic Methods},
  volume    = {4},
  url       = {https://arxiv.org/abs/1802.09477v3},
  year      = {2018}
}
@article{GarciaCespedes2025,
  abstract  = {Machine Learning models explainability has recently become a very popular topic in the banking sector. We apply Shapley values and the Python library SHAP to a real credit risk database and show that the two available SHAP types (interventional and path-dependent) provide very similar results even under correlated features. The main drawback of SHAP is that it is not portfolio invariant, this is, the explanation for the prediction provided for each observation depends on the portfolio distribution of the features. This can be a serious problem for customers and banking regulators, who expect that explanations will stay stable as long as the clients characteristics do not change. We conduct several tests and show that the SHAP explanation of an observation may considerably change depending on the rest of the portfolio distribution. As a consequence, the explanation of a client may vary over time even if her characteristics do not change and banks using the same model (for ex. commercial models) may provide different explanations to the same client.},
  author    = {Rubén García-Céspedes and Francisco J. Alias-Carrascosa and Manuel Moreno},
  doi       = {10.1080/01605682.2025.2485263},
  issn      = {14769360},
  journal   = {Journal of the Operational Research Society},
  keywords  = {Finance,Machine Learning,SHAP,credit risk},
  publisher = {Taylor and Francis Ltd.},
  title     = {On Machine Learning models explainability in the banking sector: the case of SHAP},
  year      = {2025}
}
@book{Goodfellow2016,
  title     = {Deep Learning},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  publisher = {MIT Press},
  note      = {\url{http://www.deeplearningbook.org}},
  year      = {2016}
}
@article{Greydanus2018,
  abstract  = {While deep reinforcement learning (deep RL) agents are effective at maximizing rewards, it is often unclear what strategies they use to do so. In this paper, we take a step toward explaining deep RL agents through a case study using Atari 2600 environments. In particular, we focus on using saliency maps to understand how an agent learns and executes a policy. We introduce a method for generating useful saliency maps and use it to show 1) what strong agents attend to, 2) whether agents are making decisions for the right or wrong reasons , and 3) how agents evolve during learning. We also test our method on non-expert human subjects and find that it improves their ability to reason about these agents. Overall, our results show that saliency information can provide significant insight into an RL agent's decisions and learning behavior.},
  author    = {Samuel Greydanus and Anurag Koul and Jonathan Dodge and Alan Fern},
  issn      = {2640-3498},
  journal   = {Proceedings of the 35th International Conference on Machine Learning},
  month     = {7},
  pages     = {1792-1801},
  publisher = {PMLR},
  title     = {Visualizing and Understanding Atari Agents},
  volume    = {80},
  url       = {https://proceedings.mlr.press/v80/greydanus18a.html},
  year      = {2018}
}
@article{Guan2021,
  abstract  = {Deep reinforcement learning (DRL) has been widely studied in the portfolio management task. However, it is challenging to understand a DRL-based trading strategy because of the black-box nature of deep neural networks. In this paper, we propose an empirical approach to explain the strategies of DRL agents for the portfolio management task. First, we use a linear model in hindsight as the reference model, which finds the best portfolio weights by assuming knowing actual stock returns in foresight. In particular, we use the coefficients of a linear model in hindsight as the reference feature weights. Secondly, for DRL agents, we use integrated gradients to define the feature weights, which are the coefficients between reward and features under a linear regression model. Thirdly, we study the prediction power in two cases, single-step prediction and multi-step prediction. In particular, we quantify the prediction power by calculating the linear correlations between the feature weights of a DRL agent and the reference feature weights, and similarly for machine learning methods. Finally, we evaluate a portfolio management task on Dow Jones 30 constituent stocks during 01/01/2009 to 09/01/2021. Our approach empirically reveals that a DRL agent exhibits a stronger multi-step prediction power than machine learning methods.},
  author    = {Mao Guan and Xiao Yang Liu},
  doi       = {10.1145/3490354.3494415},
  isbn      = {9781450391481},
  journal   = {ICAIF 2021 - 2nd ACM International Conference on AI in Finance},
  keywords  = {Explainable deep reinforcement learning,Integrated Gradient,Reinforcement learning,Value iteration KEYWORDS Explainable deep reinforcement learning, Integrated Gradient, lin-ear model in hindsight, portfolio management * Equal contribution † Corresponding author,linear model in hindsight,portfolio management},
  month     = {11},
  publisher = {Association for Computing Machinery, Inc},
  title     = {Explainable Deep Reinforcement Learning for Portfolio Management: An Empirical Approach},
  url       = {https://arxiv.org/abs/2111.03995v2},
  year      = {2021}
}
@article{Gunning2019,
  abstract  = {Explainability is essential for users to effectively understand, trust, and manage powerful artificial intelligence applications.},
  author    = {David Gunning and Mark Stefik and Jaesik Choi and Timothy Miller and Simone Stumpf and Guang Zhong Yang},
  doi       = {10.1126/SCIROBOTICS.AAY7120},
  issn      = {24709476},
  issue     = {37},
  journal   = {Science Robotics},
  month     = {12},
  pmid      = {33137719},
  publisher = {American Association for the Advancement of Science},
  title     = {XAI—Explainable artificial intelligence},
  volume    = {4},
  url       = {https://www.science.org/doi/10.1126/scirobotics.aay7120},
  year      = {2019}
}
@article{Haarnoja2018,
  abstract  = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  author    = {Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
  isbn      = {9781510867963},
  journal   = {35th International Conference on Machine Learning, ICML 2018},
  month     = {1},
  pages     = {2976-2989},
  publisher = {International Machine Learning Society (IMLS)},
  title     = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  volume    = {5},
  url       = {https://arxiv.org/abs/1801.01290v2},
  year      = {2018}
}
@article{HaGoogleBrainTokyo2018,
  abstract = {A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of paper: https://worldmodels.github.io},
  author   = {David Ha Google Brain Tokyo and Jürgen Schmidhuber},
  journal  = {Advances in Neural Information Processing Systems},
  title    = {Recurrent World Models Facilitate Policy Evolution},
  volume   = {31},
  url      = {https://worldmodels.github.io},
  year     = {2018}
}
@article{Hasan2024,
  abstract  = {To efficiently capture diverse fluctuation profiles in forecasting crude oil prices, we here propose to combine heterogeneous predictors for forecasting the prices of crude oil. Specifically, a forecasting model is developed using blended ensemble learning that combines various machine learning methods, including k-nearest neighbour regression, regression trees, linear regression, ridge regression, and support vector regression. Data for Brent and WTI crude oil prices at various time series frequencies are used to validate the proposed blending ensemble learning approach. To show the validity of the proposed model, its performance is further benchmarked against existing individual and ensemble learning methods used for predicting crude oil price, such as lasso regression, bagging lasso regression, boosting, random forest, and support vector regression. We demonstrate that our proposed blending-based model dominates the existing forecasting models in terms of forecasting errors for both short- and medium-term horizons.},
  author    = {Mahmudul Hasan and Mohammad Zoynul Abedin and Petr Hajek and Kristof Coussement and Md Nahid Sultan and Brian Lucey},
  doi       = {10.1007/S10479-023-05810-8/TABLES/10},
  issn      = {15729338},
  journal   = {Annals of Operations Research},
  keywords  = {Blending,Brent,Crude oil price,Ensemble learning,Forecasting,Stacking regression,WTI},
  month     = {1},
  pages     = {1-31},
  publisher = {Springer},
  title     = {A blending ensemble learning model for crude oil price forecasting},
  url       = {https://link.springer.com/article/10.1007/s10479-023-05810-8},
  year      = {2024}
}
@article{Huang2022,
  abstract = {Advantage Actor-critic (A2C) and Proximal Policy Optimization (PPO) are popular deep reinforcement learning algorithms used for game AI in recent years. A common understanding is that A2C and PPO are separate algorithms because PPO's clipped objective appears significantly different than A2C's objective. In this paper, however, we show A2C is a special case of PPO. We present theoretical justifications and pseudocode analysis to demonstrate why. To validate our claim, we conduct an empirical experiment using \texttt\{Stable-baselines3\}, showing A2C and PPO produce the \textit\{exact\} same models when other settings are controlled.},
  author   = {Shengyi Huang and Anssi Kanervisto and Antonin Raffin and Weixun Wang and Santiago Ontañón and Rousslan Fernand and Julien Dossa},
  month    = {5},
  title    = {A2C is a special case of PPO},
  url      = {https://arxiv.org/abs/2205.09123v1},
  year     = {2022}
}
@misc{IBM2021,
  author = {IBM},
  month  = {9},
  title  = {What Is Machine Learning (ML)?},
  url    = {https://www.ibm.com/think/topics/machine-learning},
  year   = {2021}
}
@article{Jiang2016,
  abstract  = {Portfolio management is the decision-making process of allocating an amount of fund into different financial investment products. Cryptocurrencies are electronic and decentralized alternatives to government-issued money, with Bitcoin as the best-known example of a cryptocurrency. This paper presents a model-less convolutional neural network with historic prices of a set of financial assets as its input, outputting portfolio weights of the set. The network is trained with 0.7 years' price data from a cryptocurrency exchange. The training is done in a reinforcement manner, maximizing the accumulative return, which is regarded as the reward function of the network. Backtest trading experiments with trading period of 30 minutes is conducted in the same market, achieving 10-fold returns in 1.8 months' periods. Some recently published portfolio selection strategies are also used to perform the same back-tests, whose results are compared with the neural network. The network is not limited to cryptocurrency, but can be applied to any other financial markets.},
  author    = {Zhengyao Jiang and Jinjun Liang},
  doi       = {10.1109/IntelliSys.2017.8324237},
  isbn      = {9781509064359},
  journal   = {2017 Intelligent Systems Conference, IntelliSys 2017},
  keywords  = {Machine learning,algorithmic trading,convolutional neural networks,cryptocurrency,deep reinforcement learning,deterministic policy gradient,portfolio management,quantitative finance},
  month     = {12},
  pages     = {905-913},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  title     = {Cryptocurrency Portfolio Management with Deep Reinforcement Learning},
  volume    = {2018-January},
  url       = {https://arxiv.org/abs/1612.01277v5},
  year      = {2016}
}

@misc{kent,
  author = {Financial Mathematics Clinic},
  editor = {University of Kent},
  pages  = {8},
  title  = {Mean Variance Portfolio Theory},
  url    = {https://www.kent.ac.uk/learning/documents/slas-documents/mean-variance-portfolio.pdf}
}
@article{Lei2020,
  abstract  = {Algorithmic trading is a continuous perception and decision making problem, where environment perception requires to learn feature representation from highly non-stationary and noisy financial time series, and decision making requires the algorithm to explore the environment and simultaneously make correct decisions in an online manner without any supervised information. To address these two problems, we propose a time-driven feature-aware jointly deep reinforcement learning model (TFJ-DRL) that integrates deep learning model and reinforcement learning model to improve the financial signal representation learning and action decision making in algorithmic trading. Concretely, we learn the environmental representation by adaptively selecting and re-weighting various features of financial signals and summarize the attention values between historical information and changing trend depending on the current state. Besides, the supervised deep learning and reinforcement learning are jointly and iteratively trained to make full use of the supervised signals in the training data, and obtain more update information and stricter loss function constraints, thereby increasing investment returns. TFJ-DRL is evaluated on real-world financial data with different price trends (rising, falling and no obvious direction). A series of analysis show the robust superiority and the extensive applicability of the proposed method.},
  author    = {Kai Lei and Bing Zhang and Yu Li and Min Yang and Ying Shen},
  doi       = {10.1016/J.ESWA.2019.112872},
  issn      = {0957-4174},
  journal   = {Expert Systems with Applications},
  keywords  = {Algorithmic trading,Deep reinforcement learning,Gate,Temporal attention},
  month     = {2},
  pages     = {112872},
  publisher = {Pergamon},
  title     = {Time-driven feature-aware jointly deep reinforcement learning for financial signal representation and algorithmic trading},
  volume    = {140},
  year      = {2020}
}
@article{Li2019,
  abstract = {Portfolio allocation is crucial for investment companies. However, getting the best strategy in a complex and dynamic stock market is challenging. In this paper, we propose a novel Adaptive Deep Deterministic Reinforcement Learning scheme (Adaptive DDPG) for the portfolio allocation task, which incorporates optimistic or pessimistic deep reinforcement learning that is reflected in the influence from prediction errors. Dow Jones 30 component stocks are selected as our trading stocks and their daily prices are used as the training and testing data. We train the Adaptive DDPG agent and obtain a trading strategy. The Adaptive DDPG's performance is compared with the vanilla DDPG, Dow Jones Industrial Average index and the traditional min-variance and mean-variance portfolio allocation strategies. Adaptive DDPG outperforms the baselines in terms of the investment return and the Sharpe ratio.},
  author   = {Xinyi Li and Yinchuan Li and Yuancheng Zhan and Xiao-Yang Liu},
  month    = {6},
  title    = {Optimistic Bull or Pessimistic Bear: Adaptive Deep Reinforcement Learning for Stock Portfolio Allocation},
  url      = {https://arxiv.org/abs/1907.01503v1},
  year     = {2019},
  journal  = {arXiv preprint arXiv:1907.01503}
}
@article{Lillicrap2015,
  abstract  = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  author    = {Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
  journal   = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
  month     = {9},
  publisher = {International Conference on Learning Representations, ICLR},
  title     = {Continuous control with deep reinforcement learning},
  url       = {https://arxiv.org/abs/1509.02971v6},
  year      = {2015}
}
@misc{LimeTabularExplainer,
  author  = {Lime},
  journal = {Documentation},
  title   = {Lime Tabular Explainer — lime 0.1 documentation},
  url     = {https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_tabular}
}
@article{Liu2018,
  abstract = {Stock trading strategy plays a crucial role in investment companies. However, it is challenging to obtain optimal strategy in the complex and dynamic stock market. We explore the potential of deep reinforcement learning to optimize stock trading strategy and thus maximize investment return. 30 stocks are selected as our trading stocks and their daily prices are used as the training and trading market environment. We train a deep reinforcement learning agent and obtain an adaptive trading strategy. The agent's performance is evaluated and compared with Dow Jones Industrial Average and the traditional min-variance portfolio allocation strategy. The proposed deep reinforcement learning approach is shown to outperform the two baselines in terms of both the Sharpe ratio and cumulative returns.},
  author   = {Xiao-Yang Liu and Zhuoran Xiong and Shan Zhong and Hongyang Yang and Anwar Walid},
  isbn     = {1811.07522v3},
  journal  = {NeurIPS 2018 AI in Finance Workshop.},
  month    = {11},
  title    = {Practical Deep Reinforcement Learning Approach for Stock Trading},
  url      = {https://arxiv.org/abs/1811.07522v3},
  year     = {2018}
}
@article{Liu2020,
  author    = {Xiao-Yang Liu},
  doi       = {10.2139/SSRN.3737257},
  journal   = {SSRN Electronic Journal},
  keywords  = {FinRL,FinRL: A Deep Reinforcement Learning Library for Automated Stock Trading in Quantitative Finance,SSRN,Xiao-Yang Liu,finance,reinforcement learning},
  month     = {11},
  publisher = {Elsevier BV},
  title     = {FinRL: A Deep Reinforcement Learning Library for Automated Stock Trading in Quantitative Finance},
  url       = {https://papers.ssrn.com/abstract=3737257},
  year      = {2020}
}
@article{Lundberg2017,
  abstract  = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  author    = {Scott M. Lundberg and Su In Lee},
  issn      = {10495258},
  journal   = {Advances in Neural Information Processing Systems},
  month     = {5},
  pages     = {4766-4775},
  publisher = {Neural information processing systems foundation},
  title     = {A Unified Approach to Interpreting Model Predictions},
  volume    = {2017-December},
  url       = {https://arxiv.org/abs/1705.07874v2},
  year      = {2017}
}
@article{Lundberg2019,
  abstract = {Interpreting predictions from tree ensemble methods such as gradient boosting machines and random forests is important, yet feature attribution for trees is often heuristic and not individualized for each prediction. Here we show that popular feature attribution methods are inconsistent, meaning they can lower a feature's assigned importance when the true impact of that feature actually increases. This is a fundamental problem that casts doubt on any comparison between features. To address it we turn to recent applications of game theory and develop fast exact tree solutions for SHAP (SHapley Additive exPlanation) values, which are the unique consistent and locally accurate attribution values. We then extend SHAP values to interaction effects and define SHAP interaction values. We propose a rich visualization of individualized feature attributions that improves over classic attribution summaries and partial dependence plots, and a unique "supervised" clustering (clustering based on feature attributions). We demonstrate better agreement with human intuition through a user study, exponential improvements in run time, improved clustering performance, and better identification of influential features. An implementation of our algorithm has also been merged into XGBoost and LightGBM, see http://github.com/slundberg/shap for details.},
  author   = {Scott M. Lundberg and Gabriel G. Erion and Su-In Lee},
  title    = {Consistent Individualized Feature Attribution for Tree Ensembles},
  url      = {https://arxiv.org/abs/1802.03888v3},
  year     = {2019},
  journal  = {Advances in Neural Information Processing Systems}
}
@article{Ma2021,
  abstract  = {In recent years, deep reinforcement learning (DRL) algorithm has been widely used in algorithmic trading. Many fully automated trading systems or strategies have been built using DRL agents, which integrate price prediction and trading signal generation in one system. However, the previous agents extract the current state from the market data without considering the long-term market historical trend when making decisions. Besides, plenty of related and useful information has not been considered. To address these two problems, we propose a novel model named Parallel Multi-Module Deep Reinforcement Learning (PMMRL) algorithm. Here, two parallel modules are used to extract and encode the feature: one module employing Fully Connected (FC) layers is used to learn the current state from the market data of the traded stock and the fundamental data of the issuing company; another module using Long Short-Term Memory (LSTM) layers aims to detect the long-term historical trend of the market. The proposed model can extract features from the whole environment by the above two modules simultaneously, taking the advantages of both LSTM and FC layers. Extensive experiments on China stock market illustrate that the proposed PMMRL algorithm achieves a higher profit and a lower drawdown than several state-of-the-art algorithms.},
  author    = {Cong Ma and Jiangshe Zhang and Junmin Liu and Lizhen Ji and Fei Gao},
  doi       = {10.1016/J.NEUCOM.2021.04.005},
  issn      = {0925-2312},
  journal   = {Neurocomputing},
  keywords  = {Capital asset pricing model,Long short-term memory,Parallel multi-module,Reinforcement learning},
  month     = {8},
  pages     = {290-302},
  publisher = {Elsevier},
  title     = {A parallel multi-module deep reinforcement learning algorithm for stock trading},
  volume    = {449},
  year      = {2021}
}
@article{Mandeep2022,
  abstract  = {The non-linear nature of the stock market prices and trends make it one of the most highly researched areas in the financial domain. People invest in the stock market based on multiple prediction techniques, classified into two main categories: classic methods like fundamental and technical analysis and AI-based prediction models. Both these techniques have their benefits and shortcomings. While the classical methods provide high interpretability, they may not be able to predict the complex trends of the stock market. AI-based models like random forests and neural networks can predict the trends with higher accuracy but provide little to no interpretability for their predictions, making them an uncertain tool for investment advice. In this paper, we use explainable artificial intelligence XAI to predict stock market trends and explain the predictions using two of the most prominent XAI tools, LIME and SHAP. The proof of concept and the experimental results are presented which show the promising application of machine learning in financial forecasting.},
  author    = {Mandeep and Abhishek Agarwal and Amrita Bhatia and Avleen Malhi and Priyal Kaler and Husanbir Singh Pannu},
  doi       = {10.1109/ICCCI55554.2022.9850272},
  isbn      = {9781665469920},
  journal   = {2022 4th International Conference on Computer Communication and the Internet, ICCCI 2022},
  keywords  = {LIME,Machine learning,SHAP,explainable artificial intelligence,financial forecasting},
  pages     = {34-38},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  title     = {Machine Learning Based Explainable Financial Forecasting},
  year      = {2022}
}
@article{Markowitz1952,
  author    = {Harry Markowitz},
  doi       = {10.2307/2975974},
  isbn      = {9789812833655},
  issn      = {00221082},
  issue     = {1},
  journal   = {The Journal of Finance},
  month     = {3},
  pages     = {77-91},
  publisher = {World Scientific Publishing Co.},
  title     = {Portfolio selection},
  volume    = {7},
  url       = {https://www.jstor.org/stable/2975974},
  year      = {1952}
}
@article{Millea2021,
  abstract  = {Deep reinforcement learning (DRL) has achieved significant results in many machine learning (ML) benchmarks. In this short survey, we provide an overview of DRL applied to trading on financial markets with the purpose of unravelling common structures used in the trading community using DRL, as well as discovering common issues and limitations of such approaches. We include also a short corpus summarization using Google Scholar. Moreover, we discuss how one can use hierarchy for dividing the problem space, as well as using model-based RL to learn a world model of the trading environment which can be used for prediction. In addition, multiple risk measures are defined and discussed, which not only provide a way of quantifying the performance of various algorithms, but they can also act as (dense) reward-shaping mechanisms for the agent. We discuss in detail the various state representations used for financial markets, which we consider critical for the success and efficiency of such DRL agents. The market in focus for this survey is the cryptocurrency market; the results of this survey are two-fold: firstly, to find the most promising directions for further research and secondly, to show how a lack of consistency in the community can significantly impede research and the development of DRL agents for trading.},
  author    = {Adrian Millea and Francisco Guijarro},
  doi       = {10.3390/DATA6110119},
  issn      = {2306-5729},
  issue     = {11},
  journal   = {Data 2021, Vol. 6, Page 119},
  keywords  = {based RL,cryptocurrency,deep reinforcement learning,foreign exchange,hierarchy,model,prediction,reward shaping,risk,stock market,trading},
  month     = {11},
  pages     = {119},
  publisher = {Multidisciplinary Digital Publishing Institute},
  title     = {Deep Reinforcement Learning for Trading—A Critical Survey},
  volume    = {6},
  url       = {https://www.mdpi.com/2306-5729/6/11/119/htm https://www.mdpi.com/2306-5729/6/11/119},
  year      = {2021}
}
@article{Mnih2013,
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  author   = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
  month    = {12},
  title    = {Playing Atari with Deep Reinforcement Learning},
  url      = {https://arxiv.org/abs/1312.5602v1},
  year     = {2013},
  journal  = {arXiv preprint arXiv:1312.5602}
}
@article{Mnih2016,
  abstract  = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  author    = {Volodymyr Mnih and Adria Puigdomenech Badia and Lehdi Mirza and Alex Graves and Tim Harley and Timothy P. Lillicrap and David Silver and Koray Kavukcuoglu},
  isbn      = {9781510829008},
  journal   = {33rd International Conference on Machine Learning, ICML 2016},
  month     = {2},
  pages     = {2850-2869},
  publisher = {International Machine Learning Society (IMLS)},
  title     = {Asynchronous Methods for Deep Reinforcement Learning},
  volume    = {4},
  url       = {https://arxiv.org/abs/1602.01783v2},
  year      = {2016}
}
@inbook{Molnar2025,
  abstract  = {Machine learning is part of our products, processes, and research. But computers usually don’t explain their predictions, which can cause many problems, ranging from trust issues to undetected bugs. This book is about making machine learning models and their decisions interpretable. After exploring the concepts of interpretability, you will learn about simple, interpretable models such as decision trees and linear regression. The focus of the book is on model-agnostic methods for interpreting black box models. Some model-agnostic methods like LIME and Shapley values can be used to explain individual predictions, while other methods like permutation feature importance and accumulated local effects can be used to get insights about the more general relations between features and predictions. In addition, the book presents methods specific to deep neural networks. All interpretation methods are explained in depth and discussed critically. How do they work? What are their strengths and weaknesses? How do you interpret them? This book will enable you to select and correctly apply the interpretation method that is most suitable for your machine learning application. Reading the book is recommended for machine learning practitioners, data scientists, statisticians, and anyone else interested in making machine learning models interpretable.},
  author    = {Christoph Molnar},
  edition   = {3},
  booktitle = {Interpretable Machine Learning},
  title     = {LIME},
  url       = {https://christophm.github.io/interpretable-ml-book/},
  year      = {2025},
  chapter   = {14},
  publisher = {Christoph Molnar}
}
@article{Moody2001,
  abstract = {We present methods for optimizing portfolios, asset allocations, and trading systems based on direct reinforcement (DR). In this approach, investment decision making is viewed as a stochastic control problem, and strategies are discovered directly. We present an adaptive algorithm called recurrent reinforcement learning (RRL) for discovering investment policies. The need to build forecasting models is eliminated, and better trading performance is obtained. The direct reinforcement approach differs from dynamic programming and reinforcement algorithms such as TD-learning and Q-learning, which attempt to estimate a value function for the control problem. We find that the RRL direct reinforcement framework enables a simpler problem representation, avoids Bellman's curse of dimensionality and offers compelling advantages in efficiency. We demonstrate how direct reinforcement can be used to optimize risk-adjusted investment returns (including the differential Sharpe ratio), while accounting for the effects of transaction costs. In extensive simulation work using real financial data, we find that our approach based on RRL produces better trading strategies than systems utilizing Q-Learning (a value function method). Real-world applications include an intra-daily currency trader and a monthly asset allocation system for the S&P 500 Stock Index and T-Bills.},
  author   = {John Moody and Matthew Saffell},
  doi      = {10.1109/72.935097},
  issn     = {10459227},
  issue    = {4},
  journal  = {IEEE Transactions on Neural Networks},
  keywords = {Differential Sharpe ratio,Direct reinforcement (DR),Downside deviation,Policy gradient,Q-learning,Recurrent reinforcement learning,Risk,TD-learning,Trading,Value function},
  month    = {7},
  pages    = {875-889},
  title    = {Learning to trade via direct reinforcement},
  volume   = {12},
  year     = {2001}
}
@article{Nembrini2018,
  abstract  = {Motivation: Random forests are fast, flexible and represent a robust approach to analyze high dimensional data. A key advantage over alternative machine learning algorithms are variable importance measures, which can be used to identify relevant features or perform variable selection. Measures based on the impurity reduction of splits, such as the Gini importance, are popular because they are simple and fast to compute. However, they are biased in favor of variables with many possible split points and high minor allele frequency. Results: We set up a fast approach to debias impurity-based variable importance measures for classification, regression and survival forests. We show that it creates a variable importance measure which is unbiased with regard to the number of categories and minor allele frequency and almost as fast as the standard impurity importance. As a result, it is now possible to compute reliable importance estimates without the extra computing cost of permutations. Further, we combine the importance measure with a fast testing procedure, producing p-values for variable importance with almost no computational overhead to the creation of the random forest. Applications to gene expression and genome-wide association data show that the proposed method is powerful and computationally efficient.},
  author    = {Stefano Nembrini and Inke R. König and Marvin N. Wright},
  doi       = {10.1093/BIOINFORMATICS/BTY373},
  issn      = {1367-4811},
  issue     = {21},
  journal   = {Bioinformatics (Oxford, England)},
  keywords  = {Algorithms*,Gene Frequency,Genome,Genome-Wide Association Study*,Inke R König,MEDLINE,Machine Learning,Marvin N Wright,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Non-U.S. Gov't,PMC6198850,PubMed Abstract,Research Support,Software,Stefano Nembrini,doi:10.1093/bioinformatics/bty373,pmid:29757357},
  month     = {11},
  pages     = {3711-3718},
  pmid      = {29757357},
  publisher = {Bioinformatics},
  title     = {The revival of the Gini importance?},
  volume    = {34},
  url       = {https://pubmed.ncbi.nlm.nih.gov/29757357/},
  year      = {2018}
}
@article{Nti2020,
  abstract  = {Stock-market prediction using machine-learning technique aims at developing effective and efficient models that can provide a better and higher rate of prediction accuracy. Numerous ensemble regressors and classifiers have been applied in stock market predictions, using different combination techniques. However, three precarious issues come in mind when constructing ensemble classifiers and regressors. The first concerns with the choice of base regressor or classifier technique adopted. The second concerns the combination techniques used to assemble multiple regressors or classifiers and the third concerns with the quantum of regressors or classifiers to be ensembled. Subsequently, the number of relevant studies scrutinising these previously mentioned concerns are limited. In this study, we performed an extensive comparative analysis of ensemble techniques such as boosting, bagging, blending and super learners (stacking). Using Decision Trees (DT), Support Vector Machine (SVM) and Neural Network (NN), we constructed twenty-five (25) different ensembled regressors and classifiers. We compared their execution times, accuracy, and error metrics over stock-data from Ghana Stock Exchange (GSE), Johannesburg Stock Exchange (JSE), Bombay Stock Exchange (BSE-SENSEX) and New York Stock Exchange (NYSE), from January 2012 to December 2018. The study outcome shows that stacking and blending ensemble techniques offer higher prediction accuracies (90–100%) and (85.7–100%) respectively, compared with that of bagging (53–97.78%) and boosting (52.7–96.32%). Furthermore, the root means square error (RMSE) recorded by stacking (0.0001–0.001) and blending (0.002–0.01) shows a better fit of ensemble classifiers and regressors based on these two techniques in market analyses compared with bagging (0.01–0.11) and boosting (0.01–0.443). Finally, the results undoubtedly suggest that an innovative study in the domain of stock market direction prediction ought to include ensemble techniques in their sets of algorithms.},
  author    = {Isaac Kofi Nti and Adebayo Felix Adekoya and Benjamin Asubam Weyori},
  doi       = {10.1186/S40537-020-00299-5/TABLES/25},
  issn      = {21961115},
  issue     = {1},
  journal   = {Journal of Big Data},
  keywords  = {Artificial intelligence,Blending,Ensemble-classifiers,Ensemble-regressors,Machine-learning,Predictions,Stacking},
  month     = {12},
  pages     = {1-40},
  publisher = {Springer Science and Business Media Deutschland GmbH},
  title     = {A comprehensive evaluation of ensemble learning for stock-market prediction},
  volume    = {7},
  url       = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00299-5},
  year      = {2020}
}
@article{Phillips2021,
  abstract = {We introduce four principles for explainable artificial intelligence (AI) that comprise fundamental properties for explainable AI systems. We propose that explainable AI systems deliver accompanying evidence or reasons for outcomes and processes; provide explanations that are understandable to individual users; provide explanations that correctly reflect the system’s process for generating the output; and that a system only operates under conditions for which it was designed and when it reaches sufficient confidence in its output. We have termed these four principles as explanation, meaningful, explanation accuracy, and knowledge limits, respectively. Through significant stakeholder engagement, these four principles were developed to encompass the multidisciplinary nature of explainable AI, including the fields of computer science, engineering, and psychology. Because one-size-fits-all explanations do not exist, different users will require different types of explanations. We present five categories of explanation and summarize theories of explainable AI. We give an overview of the algorithms in the field that cover the major classes of explainable algorithms. As a baseline comparison, we assess how well explanations provided by people follow our four principles. This assessment provides insights to the challenges of designing explainable AI systems.},
  author   = {P Jonathon Phillips and Carina A Hahn and Peter C Fontana and Amy N Yates and Kristen Greene and David A Broniatowski and Mark A Przybocki},
  doi      = {10.6028/NIST.IR.8312},
  journal  = {National Institute of Standards and Technology},
  keywords = {Artificial Intelligence (AI),explainability,explainable AI,trustworthy AI.},
  month    = {9},
  title    = {Four Principles of Explainable Artificial Intelligence},
  volume   = {Internal Report 8312},
  url      = {https://doi.org/10.6028/NIST.IR.8312},
  year     = {2021}
}
@article{Raffin2021,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  issue   = {268},
  journal = {Journal of Machine Learning Research},
  pages   = {1-8},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  volume  = {22},
  url     = {http://jmlr.org/papers/v22/20-1364.html},
  year    = {2021}
}
@article{Ribeiro2016,
  abstract  = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  author    = {Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
  doi       = {10.1145/2939672.2939778/SUPPL_FILE/KDD2016_RIBEIRO_ANY_CLASSIFIER_01-ACM.MP4},
  isbn      = {9781450342322},
  journal   = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  month     = {8},
  pages     = {1135-1144},
  publisher = {Association for Computing Machinery},
  title     = {"Why should i trust you?": Explaining the predictions of any classifier},
  url       = {https://dl.acm.org/doi/10.1145/2939672.2939778},
  year      = {2016}
}
@article{Sato2019,
  abstract = {Financial portfolio management is one of the problems that are most frequently encountered in the investment industry. Nevertheless, it is not widely recognized that both Kelly Criterion and Risk Parity collapse into Mean Variance under some conditions, which implies that a universal solution to the portfolio optimization problem could potentially exist. In fact, the process of sequential computation of optimal component weights that maximize the portfolio's expected return subject to a certain risk budget can be reformulated as a discrete-time Markov Decision Process (MDP) and hence as a stochastic optimal control, where the system being controlled is a portfolio consisting of multiple investment components, and the control is its component weights. Consequently, the problem could be solved using model-free Reinforcement Learning (RL) without knowing specific component dynamics. By examining existing methods of both value-based and policy-based model-free RL for the portfolio optimization problem, we identify some of the key unresolved questions and difficulties facing today's portfolio managers of applying model-free RL to their investment portfolios.},
  author   = {Yoshiharu Sato},
  keywords = {Portfolio Management,Quantitative Finance,Reinforcement Learning},
  month    = {4},
  title    = {Model-Free Reinforcement Learning for Financial Portfolios: A Brief Survey},
  url      = {https://arxiv.org/abs/1904.04973v2},
  journal  = {arXiv preprint arXiv:1904.04973},
  year     = {2019}
}
@article{Schulman2015,
  abstract  = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  author    = {John Schulman and Sergey Levine and Philipp Moritz and Michael Jordan and Pieter Abbeel},
  isbn      = {9781510810587},
  journal   = {32nd International Conference on Machine Learning, ICML 2015},
  month     = {2},
  pages     = {1889-1897},
  publisher = {International Machine Learning Society (IMLS)},
  title     = {Trust Region Policy Optimization},
  volume    = {3},
  url       = {https://arxiv.org/abs/1502.05477v5},
  year      = {2015}
}
@article{Schulman2017,
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  author   = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov Openai},
  month    = {7},
  title    = {Proximal Policy Optimization Algorithms},
  url      = {https://arxiv.org/abs/1707.06347v2},
  year     = {2017},
  journal  = {arXiv preprint arXiv:1707.06347}
}
@article{Sequeira2020,
  abstract  = {We propose an explainable reinforcement learning (XRL) framework that analyzes an agent's history of interaction with the environment to extract interestingness elements that help explain its behavior. The framework relies on data readily available from standard RL algorithms, augmented with data that can easily be collected by the agent while learning. We describe how to create visual summaries of an agent's behavior in the form of short video-clips highlighting key interaction moments, based on the proposed elements. We also report on a user study where we evaluated the ability of humans to correctly perceive the aptitude of agents with different characteristics, including their capabilities and limitations, given visual summaries automatically generated by our framework. The results show that the diversity of aspects captured by the different interestingness elements is crucial to help humans correctly understand an agent's strengths and limitations in performing a task, and determine when it might need adjustments to improve its performance.},
  author    = {Pedro Sequeira and Melinda Gervasio},
  doi       = {10.1016/J.ARTINT.2020.103367},
  issn      = {0004-3702},
  journal   = {Artificial Intelligence},
  keywords  = {Autonomy,Explainable AI,Interestingness elements,Reinforcement learning,Video highlights,Visual explanations},
  month     = {11},
  pages     = {103367},
  publisher = {Elsevier},
  title     = {Interestingness elements for explainable reinforcement learning: Understanding agents' capabilities and limitations},
  volume    = {288},
  year      = {2020}
}
@article{Shao2019,
  abstract = {Deep reinforcement learning (DRL) has made great achievements since proposed. Generally, DRL agents receive high-dimensional inputs at each step, and make actions according to deep-neural-network-based policies. This learning mechanism updates the policy to maximize the return with an end-to-end method. In this paper, we survey the progress of DRL methods, including value-based, policy gradient, and model-based algorithms, and compare their main techniques and properties. Besides, DRL plays an important role in game artificial intelligence (AI). We also take a review of the achievements of DRL in various video games, including classical Arcade games, first-person perspective games and multi-agent real-time strategy games, from 2D to 3D, and from single-agent to multi-agent. A large number of video game AIs with DRL have achieved super-human performance, while there are still some challenges in this domain. Therefore, we also discuss some key points when applying DRL methods to this field, including exploration-exploitation, sample efficiency, generalization and transfer, multi-agent learning, imperfect information, and delayed spare rewards, as well as some research directions.},
  author   = {Kun Shao and Zhentao Tang and Yuanheng Zhu and Nannan Li and Dongbin Zhao},
  keywords = {Index Terms-reinforcement learning,deep learning,deep reinforcement learning,game AI,video games},
  month    = {12},
  title    = {A Survey of Deep Reinforcement Learning in Video Games},
  url      = {https://arxiv.org/abs/1912.10944v2},
  year     = {2019},
  journal  = {arXiv preprint arXiv:1912.10944}
}
@misc{ShapKernelExplainer,
  author  = {SHAP},
  journal = {Documentation},
  title   = {shap.KernelExplainer — SHAP documentation},
  url     = {https://shap.readthedocs.io/en/latest/generated/shap.KernelExplainer.html#shap-kernelexplainer}
}
@inbook{Shapley1953,
  author    = {Lloyd Shapley},
  isbn      = {9781400881970},
  booktitle = {Contributions to the Theory of Games},
  pages     = {307-317},
  publisher = {Princeton University Press},
  title     = {A value for n-person games},
  year      = {1953}
}
@misc{ShapTreeExplainer,
  author  = {SHAP},
  journal = {Documentation},
  title   = {shap.TreeExplainer — SHAP documentation},
  url     = {https://shap.readthedocs.io/en/latest/generated/shap.TreeExplainer.html#shap.TreeExplainer}
}
@article{Sharpe1994,
  author  = {William Sharpe},
  journal = {The Journal of Portfolio Management},
  title   = {The Sharpe Ratio},
  volume  = {Fall},
  url     = {https://web.stanford.edu/~wfsharpe/art/sr/sr.htm},
  year    = {1994}
}
@article{Shen2020,
  abstract  = {In the era of big data, deep learning for predicting stock market prices and trends has become even more popular than before. We collected 2 years of data from Chinese stock market and proposed a comprehensive customization of feature engineering and deep learning-based model for predicting price trend of stock markets. The proposed solution is comprehensive as it includes pre-processing of the stock market dataset, utilization of multiple feature engineering techniques, combined with a customized deep learning based system for stock market price trend prediction. We conducted comprehensive evaluations on frequently used machine learning models and conclude that our proposed solution outperforms due to the comprehensive feature engineering that we built. The system achieves overall high accuracy for stock market trend prediction. With the detailed design and evaluation of prediction term lengths, feature engineering, and data pre-processing methods, this work contributes to the stock analysis research community both in the financial and technical domains.},
  author    = {Jingyi Shen and M. Omair Shafiq},
  doi       = {10.1186/S40537-020-00333-6/TABLES/9},
  issn      = {21961115},
  issue     = {1},
  journal   = {Journal of Big Data},
  keywords  = {Deep learning,Feature engineering,Prediction,Stock market trend},
  month     = {12},
  pages     = {1-33},
  publisher = {Springer Science and Business Media Deutschland GmbH},
  title     = {Short-term stock market price trend prediction using a comprehensive deep learning system},
  volume    = {7},
  url       = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00333-6},
  year      = {2020}
}
@article{Silver2016,
  abstract  = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away. A computer Go program based on deep neural networks defeats a human professional player to achieve one of the grand challenges of artificial intelligence. The victory in 1997 of the chess-playing computer Deep Blue in a six-game series against the then world champion Gary Kasparov was seen as a significant milestone in the development of artificial intelligence. An even greater challenge remained — the ancient game of Go. Despite decades of refinement, until recently the strongest computers were still playing Go at the level of human amateurs. Enter AlphaGo. Developed by Google DeepMind, this program uses deep neural networks to mimic expert players, and further improves its performance by learning from games played against itself. AlphaGo has achieved a 99% win rate against the strongest other Go programs, and defeated the reigning European champion Fan Hui 5–0 in a tournament match. This is the first time that a computer program has defeated a human professional player in even games, on a full, 19 x 19 board, in even games with no handicap.},
  author    = {David Silver and Aja Huang and Chris J. Maddison and Arthur Guez and Laurent Sifre and George Van Den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Veda Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and Timothy Lillicrap and Madeleine Leach and Koray Kavukcuoglu and Thore Graepel and Demis Hassabis},
  doi       = {10.1038/nature16961},
  issn      = {1476-4687},
  issue     = {7587},
  journal   = {Nature 2016 529:7587},
  keywords  = {Computational science,Computer science,Reward},
  month     = {1},
  pages     = {484-489},
  pmid      = {26819042},
  publisher = {Nature Publishing Group},
  title     = {Mastering the game of Go with deep neural networks and tree search},
  volume    = {529},
  url       = {https://www.nature.com/articles/nature16961},
  year      = {2016}
}
@article{Silver2017,
  abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
  author   = {David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
  month    = {12},
  title    = {Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm},
  url      = {https://arxiv.org/abs/1712.01815v1},
  year     = {2017},
  journal  = {arXiv preprint arXiv:1712.01815}
}
@misc{sklearn52,
  author  = {Scikit Learn},
  journal = {Scikit Learn Documentation},
  title   = {5.2. Permutation feature importance — scikit-learn 1.7.1 documentation},
  url     = {https://scikit-learn.org/stable/modules/permutation_importance.html#outline-of-the-permutation-importance-algorithm}
}
@misc{sklearnFeatureImportance,
  author  = {Scikit Learn},
  journal = {Documentation},
  title   = {1.11.2.5. Feature importance evaluation — scikit-learn 1.7.1 documentation},
  url     = {https://scikit-learn.org/stable/modules/ensemble.html#feature-importance-evaluation}
}
@misc{sklearnHalvingGridSearch,
  author  = {Scikit Learn},
  journal = {Documentation},
  title   = {HalvingGridSearchCV — scikit-learn 1.7.1 documentation},
  url     = {https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html#sklearn.model_selection.HalvingGridSearchCV}
}
@misc{sklearnRandomForest,
  author  = {Scikit Learn},
  journal = {Documentation},
  title   = {RandomForestRegressor — scikit-learn documentation},
  url     = {https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html}
}
@article{Strumbelj2011,
  abstract  = {We propose a method for explaining regression models and their predictions for individual instances. The method successfully reveals how individual features influence the model and can be used with any type of regression model in a uniform way. We used different...},
  author    = {Erik Štrumbelj and Igor Kononenko},
  doi       = {10.1007/978-3-642-20267-4_3},
  isbn      = {978-3-642-20267-4},
  issn      = {1611-3349},
  issue     = {PART 2},
  journal   = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords  = {Neural networks,SVM,prediction,transparency},
  pages     = {21-30},
  publisher = {Springer, Berlin, Heidelberg},
  title     = {A General Method for Visualizing and Explaining Black-Box Regression Models},
  volume    = {6594 LNCS},
  url       = {https://link.springer.com/chapter/10.1007/978-3-642-20267-4_3},
  year      = {2011}
}
@article{Tang2024,
  abstract  = {Reinforcement learning (RL), particularly its combination with deep neural networks referred to as deep RL (DRL), has shown tremendous promise across a wide range of applications, suggesting its potential for enabling the development of sophisticated robotic behaviors. Robotics problems, however, pose fundamental difficulties for the application of RL, stemming from the complexity and cost of interacting with the physical world. This article provides a modern survey of DRL for robotics, with a particular focus on evaluating the real-world successes achieved with DRL in realizing several key robotic competencies. Our analysis aims to identify the key factors underlying those exciting successes, reveal underexplored areas, and provide an overall characterization of the status of DRL in robotics. We highlight several important avenues for future work, emphasizing the need for stable and sample-efficient real-world RL paradigms, holistic approaches for discovering and integrating various competencies to tackle complex long-horizon, open-world tasks, and principled development and evaluation procedures. This survey is designed to offer insights for both RL practitioners and roboticists toward harnessing RL's power to create generally capable real-world robotic systems.},
  author    = {Chen Tang and Ben Abbatematteo and Jiaheng Hu and Rohan Chandra and Roberto Martín-Martín and Peter Stone},
  doi       = {10.1146/annurev-control-030323-022510},
  issn      = {25735144},
  issue     = {1},
  journal   = {Annual Review of Control, Robotics, and Autonomous Systems},
  keywords  = {deep learning,learning for control,real-world applications,reinforcement learning,robotics},
  month     = {8},
  pages     = {153-188},
  publisher = {Annual Reviews Inc.},
  title     = {Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes},
  volume    = {8},
  url       = {https://arxiv.org/abs/2408.03539v3},
  year      = {2024}
}
@techreport{Thrun1992,
  author      = {Sebastian Thrun},
  city        = {Pittsburgh, PA},
  institution = {Carnegie Mellon University},
  month       = {1},
  title       = {Efficient Exploration In Reinforcement Learning},
  url         = {https://www.ri.cmu.edu/publications/efficient-exploration-in-reinforcement-learning/},
  year        = {1992}
}
@article{Uhlenbeck1930,
  abstract  = {With a method first indicated by Ornstein the mean values of all the powers of the velocity u and the displacement s of a free particle in Brownian motion are calculated. It is shown that u-u0exp(-βt) and s-u0β[1-exp(-βt)] where u0 is the initial velocity and β the friction coefficient divided by the mass of the particle, follow the normal Gaussian distribution law. For s this gives the exact frequency distribution corresponding to the exact formula for s2 of Ornstein and Fürth. Discussion is given of the connection with the Fokker-Planck partial differential equation. By the same method exact expressions are obtained for the square of the deviation of a harmonically bound particle in Brownian motion as a function of the time and the initial deviation. Here the periodic, aperiodic and overdamped cases have to be treated separately. In the last case, when β is much larger than the frequency and for values of tβ-1, the formula takes the form of that previously given by Smoluchowski. © 1930 The American Physical Society.},
  author    = {G. E. Uhlenbeck and L. S. Ornstein},
  doi       = {10.1103/PhysRev.36.823},
  issn      = {0031899X},
  issue     = {5},
  journal   = {Physical Review},
  month     = {9},
  pages     = {823},
  publisher = {American Physical Society},
  title     = {On the Theory of the Brownian Motion},
  volume    = {36},
  url       = {https://journals.aps.org/pr/abstract/10.1103/PhysRev.36.823},
  year      = {1930}
}
@misc{wandb,
  title  = {Experiment Tracking with Weights and Biases},
  year   = {2020},
  note   = {Software available from wandb.com},
  url    = {https://www.wandb.com/},
  author = {Biewald, Lukas}
}
@misc{WeightsBiases2025,
  author  = {Weights & Biases},
  journal = {Documentation},
  month   = {8},
  title   = {Stable Baselines 3 | Weights \& Biases Documentation},
  url     = {https://docs.wandb.ai/guides/integrations/stable-baselines-3/},
  year    = {2025}
}
@article{Wu2017,
  abstract  = {In this work, we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also a method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the MuJoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2- to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at https://github.com/openai/baselines},
  author    = {Yuhuai Wu and Elman Mansimov and Shun Liao and Roger Grosse and Jimmy Ba},
  issn      = {10495258},
  journal   = {Advances in Neural Information Processing Systems},
  month     = {8},
  pages     = {5280-5289},
  publisher = {Neural information processing systems foundation},
  title     = {Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation},
  volume    = {2017-December},
  url       = {https://arxiv.org/abs/1708.05144v2},
  year      = {2017}
}
@article{Wu2023,
  abstract  = {In today’s society, investment wealth management has become a mainstream of the contemporary era. Investment wealth management refers to the use of funds by investors to arrange funds reasonably, for example, savings, bank financial products, bonds, stocks, commodity spots, real estate, gold, art, and many others. Wealth management tools manage and assign families, individuals, enterprises, and institutions to achieve the purpose of increasing and maintaining value to accelerate asset growth. Among them, in investment and financial management, people’s favorite product of investment often stocks, because the stock market has great advantages and charm, especially compared with other investment methods. More and more scholars have developed methods of prediction from multiple angles for the stock market. According to the feature of financial time series and the task of price prediction, this article proposes a new framework structure to achieve a more accurate prediction of the stock price, which combines Convolution Neural Network (CNN) and Long–Short-Term Memory Neural Network (LSTM). This new method is aptly named stock sequence array convolutional LSTM (SACLSTM). It constructs a sequence array of historical data and its leading indicators (options and futures), and uses the array as the input image of the CNN framework, and extracts certain feature vectors through the convolutional layer and the layer of pooling, and as the input vector of LSTM, and takes ten stocks in U.S.A and Taiwan as the experimental data. Compared with previous methods, the prediction performance of the proposed algorithm in this article leads to better results when compared directly.},
  author    = {Jimmy Ming Tai Wu and Zhongcui Li and Norbert Herencsar and Bay Vo and Jerry Chun Wei Lin},
  doi       = {10.1007/S00530-021-00758-W/TABLES/15},
  issn      = {14321882},
  issue     = {3},
  journal   = {Multimedia Systems},
  keywords  = {Convolution neural network,Leading indicators,Long–short-term memory neural network,Stock price prediction},
  month     = {6},
  pages     = {1751-1770},
  publisher = {Springer Science and Business Media Deutschland GmbH},
  title     = {A graph-based CNN-LSTM stock price prediction algorithm with leading indicators},
  volume    = {29},
  url       = {https://link.springer.com/article/10.1007/s00530-021-00758-w},
  year      = {2023}
}
@article{Yang2020,
  abstract  = {Stock trading strategies play a critical role in investment. However, it is challenging to design a profitable strategy in a complex and dynamic stock market. In this paper, we propose an ensemble strategy that employs deep reinforcement schemes to learn a stock trading strategy by maximizing investment return. We train a deep reinforcement learning agent and obtain an ensemble trading strategy using three actor-critic based algorithms: Proximal Policy Optimization (PPO), Advantage Actor Critic (A2C), and Deep Deterministic Policy Gradient (DDPG). The ensemble strategy inherits and integrates the best features of the three algorithms, thereby robustly adjusting to different market situations. In order to avoid the large memory consumption in training networks with continuous action space, we employ a load-on-demand technique for processing very large data. We test our algorithms on the 30 Dow Jones stocks that have adequate liquidity. The performance of the trading agent with different reinforcement learning algorithms is evaluated and compared with both the Dow Jones Industrial Average index and the traditional min-variance portfolio allocation strategy. The proposed deep ensemble strategy is shown to outperform the three individual algorithms and two baselines in terms of the risk-adjusted return measured by the Sharpe ratio.},
  author    = {Hongyang Yang and Xiao Yang Liu and Shan Zhong and Anwar Walid},
  doi       = {10.1145/3383455.3422540},
  isbn      = {9781450375849},
  journal   = {ICAIF 2020 - 1st ACM International Conference on AI in Finance},
  keywords  = {Actor-critic framework,Automated stock trading,Deep reinforcement learning,Ensemble strategy,Markov decision process},
  month     = {10},
  publisher = {Association for Computing Machinery, Inc},
  title     = {Deep reinforcement learning for automated stock trading: An ensemble strategy},
  url       = {https://dl.acm.org/doi/10.1145/3383455.3422540},
  year      = {2020}
}
@article{Zhang2019,
  abstract  = {We adopt Deep Reinforcement Learning algorithms to design trading strategies for continuous futures contracts. Both discrete and continuous action spaces are considered and volatility scaling is incorporated to create reward functions which scale trade positions based on market volatility. We test our algorithms on the 50 most liquid futures contracts from 2011 to 2019, and investigate how performance varies across different asset classes including commodities, equity indices, fixed income and FX markets. We compare our algorithms against classical time series momentum strategies, and show that our method outperforms such baseline models, delivering positive profits despite heavy transaction costs. The experiments show that the proposed algorithms can follow large market trends without changing positions and can also scale down, or hold, through consolidation periods.},
  author    = {Zihao Zhang and Stefan Zohren and Stephen Roberts},
  journal   = {Papers},
  publisher = {arXiv.org},
  title     = {Deep Reinforcement Learning for Trading},
  url       = {https://ideas.repec.org/p/arx/papers/1911.10107.html https://ideas.repec.org//p/arx/papers/1911.10107.html},
  year      = {2019}
}
@article{Zhang2022,
  abstract  = {Artificial Intelligence (AI) and Machine Learning (ML) are gaining increasing attention regarding their potential applications in auditing. One major challenge of their adoption in auditing is the lack of explainability of their results. As AI/ML matures, so do techniques that can enhance the interpretability of AI, a.k.a., Explainable Artificial Intelligence (XAI). This paper introduces XAI techniques to auditing practitioners and researchers. We discuss how different XAI techniques can be used to meet the requirements of audit documentation and audit evidence standards. Furthermore, we demonstrate popular XAI techniques, especially Local Interpretable Model-agnostic Explanations (LIME) and Shapley Additive exPlanations (SHAP), using an auditing task of assessing the risk of material misstatement. This paper contributes to accounting information systems research and practice by introducing XAI techniques to enhance the transparency and interpretability of AI applications applied to auditing tasks.},
  author    = {Chanyuan (Abigail) Zhang and Soohyun Cho and Miklos Vasarhelyi},
  doi       = {10.1016/J.ACCINF.2022.100572},
  issn      = {1467-0895},
  journal   = {International Journal of Accounting Information Systems},
  keywords  = {Auditing,Explainable Artificial Intelligence (XAI),LIME,Machine learning,Material restatement,SHAP},
  month     = {9},
  pages     = {100572},
  publisher = {Pergamon},
  title     = {Explainable Artificial Intelligence (XAI) in auditing},
  volume    = {46},
  year      = {2022}
}
@article{Zhang2023,
  abstract  = {Accurately predicting the prices of financial time series is essential and challenging for the financial sector. Owing to recent advancements in deep learning techniques, deep learning models are gradually replacing traditional statistical and machine learning models as the first choice for price forecasting tasks. This shift in model selection has led to a notable rise in research related to applying deep learning models to price forecasting, resulting in a rapid accumulation of new knowledge. Therefore, we conducted a literature review of relevant studies over the past 3 years with a view to aiding researchers and practitioners in the field. This review delves deeply into deep learning-based forecasting models, presenting information on model architectures, practical applications, and their respective advantages and disadvantages. In particular, detailed information is provided on advanced models for price forecasting, such as Transformers, generative adversarial networks (GANs), graph neural networks (GNNs), and deep quantum neural networks (DQNNs). The present contribution also includes potential directions for future research, such as examining the effectiveness of deep learning models with complex structures for price forecasting, extending from point prediction to interval prediction using deep learning models, scrutinizing the reliability and validity of decomposition ensembles, and exploring the influence of data volume on model performance. This article is categorized under: Technologies > Prediction Technologies > Artificial Intelligence.},
  author    = {Cheng Zhang and Nilam Nur Amir Sjarif and Roslina Ibrahim},
  doi       = {10.1002/WIDM.1519},
  issn      = {1942-4795},
  issue     = {1},
  journal   = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  keywords  = {deep learning,financial market,neural network,price forecast,time series},
  month     = {9},
  pages     = {e1519},
  publisher = {John Wiley \& Sons, Ltd},
  title     = {Deep learning models for price forecasting of financial time series: A review of recent advancements: 2020–2022},
  volume    = {14},
  url       = {https://onlinelibrary.wiley.com/doi/full/10.1002/widm.1519 https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1519 https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.1519},
  year      = {2023}
}
