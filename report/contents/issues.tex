\chapter{Legal, Social, Ethical and Professional Issues} \label{ch:issues}

The development and deployment of \acrfull{drl} algorithms to perform profitable portfolio allocation raises several legal, social, ethical and professional issues that must be considered. The incorporation of post-hoc explainable techniques to understand, interpret and clarify the decision-making process of these algorithms is a crucial step towards addressing these. This chapter will explore these concerns in detail, highlighting remediating circumstances and potential solutions.

\section{Legal Issues} \label{sec:legal-issues}

When it comes to the legal ramifications of deploying \acrfull{ml} algorithms in finance, under the \acrfull{eu}'s \acrfull{aiact} \cite{AIAct2024} their use can be classified as high-risk per Article 6 \footnote{https://artificialintelligenceact.eu/article/6/} due to their potential impact on an individuals' economic well-being. Consequently, such systems must comply with the requirements set in Section 2, Articles 9-15 \footnote{https://artificialintelligenceact.eu/section/3-2/}, which outline the obligations for high-risk AI systems, including risk assessment, data governance, and transparency measures. Particularly, to comply with Article 13 \footnote{https://artificialintelligenceact.eu/article/13/}, which outlines that high-risk AI systems must "enable deployers to interpret a system's output", the use of explainability techniques is essential. This ensures that the decision-making process of the algorithm can be understood and justified, thus adhering to the legal requirements set forth by the \acrshort{aiact}.

Additionally, \Gls{algorithmictrading} systems must comply with the \acrlong{eu} directive on \acrfull{mifid} \cite{MiFIDII} and the \acrfull{fca} guidance on \textit{Algorithmic Trading Compliance in Wholesale Markets}. These regulations outline that algorithmic decision-making processes must be subject to risk controls and a governance and oversight framework, as well as robust and consistent development and testing processes, amongst others. 

Consequently, the deployment of this particular implementation of portfolio optimisation using \acrfull{drl} must ensure that it follows adequate testing procedures and implements the necessary safeguards to mitigate potential risks. Moreover, by already providing post-hoc explainability techniques, it ensures that the decision-making process can be interpreted and justified. 

\section{Social Issues} \label{sec:social-issues}

Despite the recent popularity of \acrfull{ai} systems with the rise of \acrfull{llm}, there is still a significant lack of understanding and trust in these systems. Some of the reasons behind the psychological reasons why people would be hesitant to fully embrace \acrshort{ai} systems include:
\begin{itemize}
    \item Black box behaviour, making it difficult for people to understand their decisions.
    \item Lacking human-like qualities, with a preference in users to interact with the human counterparty.
    \item Perceived inability of the systems to adapt to new situations and learn from previous mistakes.
\end{itemize}

For our particular use case, the incorporation of explainability techniques addresses the black box behaviour of the implemented technology. However, there is a difference between generating the explanations and the end-user being able to access and understand them. To bridge this gap, it is essential to provide a user-friendly interface that allows users to easily access the explanations and provides human language descriptions of the explanations. 

% MAYBE: Another important aspect to consider with regards to social issues is the employment impact of automated systems.
