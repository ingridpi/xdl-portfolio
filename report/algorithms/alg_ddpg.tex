\begin{algorithmic}
\State \textbf{Initialise:}
\State \quad Critic network $Q_{\theta^Q}(s, a)$ and actor $\mu_{\theta^\mu}(s)$ with random weights $\theta^Q$, $\theta^\mu$
\State \quad Target networks: $\theta^{Q'} \leftarrow \theta^Q$, $\theta^{\mu'} \leftarrow \theta^\mu$
\State \quad Replay buffer $\mathcal{B}$
\State \quad Hyper-parameters: discount $\gamma$, soft update rate $\tau$, batch size $N$, exploration noise process $\mathcal{N}$, learning rates $\alpha_Q, \alpha_\mu$, total episodes $M$, steps per episode $T$
\For{episode $= 1$ to $M$}
    \State Initialise random process $\mathcal{N}$ for action exploration
    \State Receive initial state $s_1$
    \For{$t = 1$ to $T$}
        \State Select action $a_t = \mu_{\theta^\mu}(s_t) + \mathcal{N}_t$
        \State Execute $a_t$, observe reward $r_t$ and next state $s_{t+1}$
        \State Store $(s_t, a_t, r_t, s_{t+1})$ in $\mathcal{B}$
        \State Sample mini-batch $\{(s_i, a_i, r_i, s_{i+1})\}_{i=1}^N$ from $\mathcal{B}$
        \State Compute target: $y_i = r_i + \gamma Q_{\theta^{Q'}}(s_{i+1}, \mu_{\theta^{\mu'}}(s_{i+1}))$
        \State Update critic by minimising: $L(\theta^Q) = \frac{1}{N}\sum_i (y_i - Q_{\theta^Q}(s_i, a_i))^2$
        \State Update actor by policy gradient:
        \State \quad $\nabla_{\theta^\mu} J \approx \frac{1}{N}\sum_i \nabla_a Q_{\theta^Q}(s_i, a)|_{a=\mu_{\theta^\mu}(s_i)} \nabla_{\theta^\mu}\mu_{\theta^\mu}(s_i)$
        \State Update target networks:
        \State \quad $\theta^{Q'} \leftarrow \tau \theta^Q + (1 - \tau)\theta^{Q'}$
        \State \quad $\theta^{\mu'} \leftarrow \tau \theta^\mu + (1 - \tau)\theta^{\mu'}$
    \EndFor
\EndFor
\end{algorithmic}