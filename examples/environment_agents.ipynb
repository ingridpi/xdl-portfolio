{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c1d5126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae10c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config\n",
    "from config import config_indicators\n",
    "\n",
    "from preprocessor.findata_preprocessor import FinancialDataPreprocessor\n",
    "from visualiser.findata_visualiser import FinancialDataVisualiser\n",
    "from environments.env_finrl import FinRLTradingEnv\n",
    "from agents.agent_finrl import FinRLAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b58ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"../\" + config.MODELS_DIR):\n",
    "    os.makedirs(\"../\" + config.MODELS_DIR)\n",
    "\n",
    "if not os.path.exists(\"../\" + config.RESULTS_DIR):\n",
    "    os.makedirs(\"../\" + config.RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be7a7dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "finpreprocessor = FinancialDataPreprocessor(config.START_DATE, config.END_DATE)\n",
    "train_data, trade_data = finpreprocessor.load_train_test_data(\n",
    "    \"../\" + config.DATA_DIR, config.TEST_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9799975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ingridperez/Documents/GitHub Repositiories/xdl-portfolio/examples/../visualiser/findata_visualiser.py:219: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "finvisualiser = FinancialDataVisualiser()\n",
    "finvisualiser.plot_train_test_close_prices(\n",
    "    train_data, trade_data, \"../\" + config.PLOT_DIR, config.TEST_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cad0aca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment successfully created with \n",
      "\tStock dimension: 5 \n",
      "\tState space: 91\n"
     ]
    }
   ],
   "source": [
    "environment = FinRLTradingEnv(\n",
    "    train_data, trade_data, list(config_indicators.TECHNICAL_INDICATORS.keys())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffbc8c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_train = environment.get_train_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca894935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Logging to ../results/a2c\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 710          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 0            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.14        |\n",
      "|    explained_variance | -1.7         |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | 10.1         |\n",
      "|    reward             | 0.12546048   |\n",
      "|    reward_max         | 0.40224797   |\n",
      "|    reward_mean        | -0.026924636 |\n",
      "|    reward_min         | -0.6772546   |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 3.16         |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 726          |\n",
      "|    iterations         | 200          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 1000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.14        |\n",
      "|    explained_variance | 0.311        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 199          |\n",
      "|    policy_loss        | 5.65         |\n",
      "|    reward             | 0.021586305  |\n",
      "|    reward_max         | 1.5162476    |\n",
      "|    reward_mean        | 0.0006147336 |\n",
      "|    reward_min         | -1.7046627   |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 1.11         |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 732         |\n",
      "|    iterations         | 300         |\n",
      "|    time_elapsed       | 2           |\n",
      "|    total_timesteps    | 1500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.13       |\n",
      "|    explained_variance | -0.297      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 299         |\n",
      "|    policy_loss        | -1.03       |\n",
      "|    reward             | -0.38687855 |\n",
      "|    reward_max         | 3.1013298   |\n",
      "|    reward_mean        | 0.5460795   |\n",
      "|    reward_min         | -2.0782287  |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.85        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 735        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.15      |\n",
      "|    explained_variance | -0.00343   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 6.6        |\n",
      "|    reward             | 1.6688919  |\n",
      "|    reward_max         | 4.897261   |\n",
      "|    reward_mean        | 1.0327803  |\n",
      "|    reward_min         | -2.5022616 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.65       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 735         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 3           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.16       |\n",
      "|    explained_variance | -0.0897     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | 10.6        |\n",
      "|    reward             | 0.32818374  |\n",
      "|    reward_max         | 0.32818374  |\n",
      "|    reward_mean        | -0.13353254 |\n",
      "|    reward_min         | -0.615585   |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 5.25        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 735        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.15      |\n",
      "|    explained_variance | 0.204      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 11.5       |\n",
      "|    reward             | 0.7613068  |\n",
      "|    reward_max         | 0.7613068  |\n",
      "|    reward_mean        | -1.0272677 |\n",
      "|    reward_min         | -4.0120134 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.46       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 735        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.17      |\n",
      "|    explained_variance | -0.0589    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -25.4      |\n",
      "|    reward             | -5.151108  |\n",
      "|    reward_max         | 0.34828094 |\n",
      "|    reward_mean        | -2.87906   |\n",
      "|    reward_min         | -5.151108  |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 14.5       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 736         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.17       |\n",
      "|    explained_variance | 0.0367      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | 22.3        |\n",
      "|    reward             | 0.42702878  |\n",
      "|    reward_max         | 2.00188     |\n",
      "|    reward_mean        | 1.06649     |\n",
      "|    reward_min         | -0.65244484 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 10.8        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 735        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.19      |\n",
      "|    explained_variance | -1.11      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 0.675      |\n",
      "|    reward             | 0.24933907 |\n",
      "|    reward_max         | 0.259528   |\n",
      "|    reward_mean        | 0.09336495 |\n",
      "|    reward_min         | -0.1760163 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.0653     |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 735          |\n",
      "|    iterations         | 1000         |\n",
      "|    time_elapsed       | 6            |\n",
      "|    total_timesteps    | 5000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.21        |\n",
      "|    explained_variance | 0.627        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 999          |\n",
      "|    policy_loss        | 0.588        |\n",
      "|    reward             | -0.7402403   |\n",
      "|    reward_max         | 0.8014741    |\n",
      "|    reward_mean        | -0.013705593 |\n",
      "|    reward_min         | -0.7402403   |\n",
      "|    std                | 1.03         |\n",
      "|    value_loss         | 0.0171       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 733         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 7           |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.25       |\n",
      "|    explained_variance | 0.0162      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | 47.2        |\n",
      "|    reward             | 2.0670989   |\n",
      "|    reward_max         | 4.7285943   |\n",
      "|    reward_mean        | -0.47864416 |\n",
      "|    reward_min         | -8.463003   |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 35          |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 732        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.27      |\n",
      "|    explained_variance | -0.226     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 10.2       |\n",
      "|    reward             | -0.4589272 |\n",
      "|    reward_max         | 2.975748   |\n",
      "|    reward_mean        | 0.89159065 |\n",
      "|    reward_min         | -0.4589272 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 3.84       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 732         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.3        |\n",
      "|    explained_variance | -0.971      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | -2.7        |\n",
      "|    reward             | -0.16310279 |\n",
      "|    reward_max         | 0.2971313   |\n",
      "|    reward_mean        | -0.09016977 |\n",
      "|    reward_min         | -0.58622324 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 0.133       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 732        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.31      |\n",
      "|    explained_variance | 0.297      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 6.54       |\n",
      "|    reward             | -0.5486204 |\n",
      "|    reward_max         | 0.48220468 |\n",
      "|    reward_mean        | 0.09978574 |\n",
      "|    reward_min         | -0.5486204 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.686      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 733         |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 10          |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.31       |\n",
      "|    explained_variance | -0.272      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | 43.9        |\n",
      "|    reward             | 0.6491343   |\n",
      "|    reward_max         | 1.262856    |\n",
      "|    reward_mean        | -0.18205048 |\n",
      "|    reward_min         | -3.2074947  |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 50          |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 734         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 10          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.33       |\n",
      "|    explained_variance | 0.303       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | -0.316      |\n",
      "|    reward             | -0.22187577 |\n",
      "|    reward_max         | -0.22187577 |\n",
      "|    reward_mean        | -1.1762638  |\n",
      "|    reward_min         | -2.0093865  |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.359       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 734         |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 11          |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.35       |\n",
      "|    explained_variance | 0.000339    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | -1.24       |\n",
      "|    reward             | -0.13892102 |\n",
      "|    reward_max         | 0.81162477  |\n",
      "|    reward_mean        | 0.21833727  |\n",
      "|    reward_min         | -0.13892102 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.147       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 735         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 12          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.38       |\n",
      "|    explained_variance | 0.251       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -29.2       |\n",
      "|    reward             | 1.0759467   |\n",
      "|    reward_max         | 2.0042646   |\n",
      "|    reward_mean        | 1.0590727   |\n",
      "|    reward_min         | -0.44798326 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 14.8        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 735        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.37      |\n",
      "|    explained_variance | -0.0495    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -1.49      |\n",
      "|    reward             | -0.4516188 |\n",
      "|    reward_max         | 5.504043   |\n",
      "|    reward_mean        | 1.2990073  |\n",
      "|    reward_min         | -4.566867  |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 31.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 735        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.34      |\n",
      "|    explained_variance | 0.0973     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -13.1      |\n",
      "|    reward             | 1.4692546  |\n",
      "|    reward_max         | 2.6683953  |\n",
      "|    reward_mean        | 0.6894899  |\n",
      "|    reward_min         | -2.9583073 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 11.4       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 736          |\n",
      "|    iterations         | 2100         |\n",
      "|    time_elapsed       | 14           |\n",
      "|    total_timesteps    | 10500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.34        |\n",
      "|    explained_variance | -284         |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2099         |\n",
      "|    policy_loss        | -6.59        |\n",
      "|    reward             | 0.04073063   |\n",
      "|    reward_max         | 0.08591073   |\n",
      "|    reward_mean        | 0.038049426  |\n",
      "|    reward_min         | -0.011044112 |\n",
      "|    std                | 1.05         |\n",
      "|    value_loss         | 1.56         |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 736          |\n",
      "|    iterations         | 2200         |\n",
      "|    time_elapsed       | 14           |\n",
      "|    total_timesteps    | 11000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.36        |\n",
      "|    explained_variance | -2.91        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2199         |\n",
      "|    policy_loss        | 1.54         |\n",
      "|    reward             | 0.0427747    |\n",
      "|    reward_max         | 0.42818448   |\n",
      "|    reward_mean        | -0.050015528 |\n",
      "|    reward_min         | -0.4524947   |\n",
      "|    std                | 1.05         |\n",
      "|    value_loss         | 0.204        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 736        |\n",
      "|    iterations         | 2300       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 11500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.32      |\n",
      "|    explained_variance | 0.0509     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2299       |\n",
      "|    policy_loss        | 13.4       |\n",
      "|    reward             | 2.0484934  |\n",
      "|    reward_max         | 2.0484934  |\n",
      "|    reward_mean        | -0.7225088 |\n",
      "|    reward_min         | -3.4002628 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 5.21       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 737         |\n",
      "|    iterations         | 2400        |\n",
      "|    time_elapsed       | 16          |\n",
      "|    total_timesteps    | 12000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.33       |\n",
      "|    explained_variance | -1.27       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2399        |\n",
      "|    policy_loss        | 20.4        |\n",
      "|    reward             | -0.92312354 |\n",
      "|    reward_max         | 2.6012821   |\n",
      "|    reward_mean        | 0.43190426  |\n",
      "|    reward_min         | -0.92312354 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 8.2         |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 736         |\n",
      "|    iterations         | 2500        |\n",
      "|    time_elapsed       | 16          |\n",
      "|    total_timesteps    | 12500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.34       |\n",
      "|    explained_variance | -0.182      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2499        |\n",
      "|    policy_loss        | 2.65        |\n",
      "|    reward             | -0.21867971 |\n",
      "|    reward_max         | 0.745409    |\n",
      "|    reward_mean        | 0.03919765  |\n",
      "|    reward_min         | -0.34915113 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.297       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 736        |\n",
      "|    iterations         | 2600       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 13000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.32      |\n",
      "|    explained_variance | 0.191      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2599       |\n",
      "|    policy_loss        | 30.5       |\n",
      "|    reward             | 1.4648772  |\n",
      "|    reward_max         | 2.7543318  |\n",
      "|    reward_mean        | 0.77639    |\n",
      "|    reward_min         | -1.1934922 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 19.1       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 737         |\n",
      "|    iterations         | 2700        |\n",
      "|    time_elapsed       | 18          |\n",
      "|    total_timesteps    | 13500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.3        |\n",
      "|    explained_variance | -0.0925     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2699        |\n",
      "|    policy_loss        | 8.29        |\n",
      "|    reward             | -1.7446971  |\n",
      "|    reward_max         | 1.1276201   |\n",
      "|    reward_mean        | -0.28355557 |\n",
      "|    reward_min         | -1.7446971  |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 2.06        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 737        |\n",
      "|    iterations         | 2800       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 14000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.29      |\n",
      "|    explained_variance | -0.422     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2799       |\n",
      "|    policy_loss        | -9.49      |\n",
      "|    reward             | -1.5118948 |\n",
      "|    reward_max         | 4.9607186  |\n",
      "|    reward_mean        | 1.0364445  |\n",
      "|    reward_min         | -1.5118948 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 4.17       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 737         |\n",
      "|    iterations         | 2900        |\n",
      "|    time_elapsed       | 19          |\n",
      "|    total_timesteps    | 14500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.28       |\n",
      "|    explained_variance | -0.129      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2899        |\n",
      "|    policy_loss        | -13.7       |\n",
      "|    reward             | 0.0750088   |\n",
      "|    reward_max         | 1.5483508   |\n",
      "|    reward_mean        | 0.28121993  |\n",
      "|    reward_min         | -0.69292504 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 4.72        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 736        |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.3       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | 2.72       |\n",
      "|    reward             | -7.066849  |\n",
      "|    reward_max         | 3.484651   |\n",
      "|    reward_mean        | -0.9421728 |\n",
      "|    reward_min         | -7.066849  |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 8.35       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 736        |\n",
      "|    iterations         | 3100       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 15500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.29      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3099       |\n",
      "|    policy_loss        | -14.4      |\n",
      "|    reward             | -2.9220316 |\n",
      "|    reward_max         | 4.8655705  |\n",
      "|    reward_mean        | 1.3689327  |\n",
      "|    reward_min         | -2.9220316 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 9.06       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 735        |\n",
      "|    iterations         | 3200       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 16000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.29      |\n",
      "|    explained_variance | 0.00454    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3199       |\n",
      "|    policy_loss        | -42.1      |\n",
      "|    reward             | -3.570414  |\n",
      "|    reward_max         | 4.9621305  |\n",
      "|    reward_mean        | -1.3724916 |\n",
      "|    reward_min         | -6.255708  |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 73.2       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 734         |\n",
      "|    iterations         | 3300        |\n",
      "|    time_elapsed       | 22          |\n",
      "|    total_timesteps    | 16500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.3        |\n",
      "|    explained_variance | 0.0451      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3299        |\n",
      "|    policy_loss        | -7.64       |\n",
      "|    reward             | -1.9926622  |\n",
      "|    reward_max         | 0.92742306  |\n",
      "|    reward_mean        | -0.25215852 |\n",
      "|    reward_min         | -1.9926622  |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 1.62        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 734         |\n",
      "|    iterations         | 3400        |\n",
      "|    time_elapsed       | 23          |\n",
      "|    total_timesteps    | 17000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.3        |\n",
      "|    explained_variance | -0.156      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3399        |\n",
      "|    policy_loss        | -56.7       |\n",
      "|    reward             | 1.0587002   |\n",
      "|    reward_max         | 4.409109    |\n",
      "|    reward_mean        | -0.57091594 |\n",
      "|    reward_min         | -8.524986   |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 55.4        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 733         |\n",
      "|    iterations         | 3500        |\n",
      "|    time_elapsed       | 23          |\n",
      "|    total_timesteps    | 17500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.28       |\n",
      "|    explained_variance | 0.00305     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3499        |\n",
      "|    policy_loss        | 77.7        |\n",
      "|    reward             | -0.23955359 |\n",
      "|    reward_max         | 0.6832867   |\n",
      "|    reward_mean        | -1.5464059  |\n",
      "|    reward_min         | -4.6533866  |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 172         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 732       |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.27     |\n",
      "|    explained_variance | -0.00757  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | -91.4     |\n",
      "|    reward             | 12.982105 |\n",
      "|    reward_max         | 12.982105 |\n",
      "|    reward_mean        | -1.877596 |\n",
      "|    reward_min         | -19.44249 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 234       |\n",
      "-------------------------------------\n",
      "day: 2011, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7068472.64\n",
      "total_reward: 6068472.64\n",
      "total_cost: 66698.15\n",
      "total_trades: 8821\n",
      "Sharpe: 1.074\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 728         |\n",
      "|    iterations         | 3700        |\n",
      "|    time_elapsed       | 25          |\n",
      "|    total_timesteps    | 18500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.27       |\n",
      "|    explained_variance | -0.699      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3699        |\n",
      "|    policy_loss        | 11          |\n",
      "|    reward             | 0.368763    |\n",
      "|    reward_max         | 0.4780874   |\n",
      "|    reward_mean        | 0.15605979  |\n",
      "|    reward_min         | -0.16986239 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 2           |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 727        |\n",
      "|    iterations         | 3800       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 19000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.27      |\n",
      "|    explained_variance | 0.22       |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3799       |\n",
      "|    policy_loss        | 6.61       |\n",
      "|    reward             | 2.0131533  |\n",
      "|    reward_max         | 2.0131533  |\n",
      "|    reward_mean        | -0.1348973 |\n",
      "|    reward_min         | -1.9356005 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 1.21       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 724        |\n",
      "|    iterations         | 3900       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 19500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.25      |\n",
      "|    explained_variance | -0.106     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3899       |\n",
      "|    policy_loss        | 17.8       |\n",
      "|    reward             | -1.8246142 |\n",
      "|    reward_max         | 4.2998905  |\n",
      "|    reward_mean        | 1.1463166  |\n",
      "|    reward_min         | -1.8246142 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 34.4       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 722         |\n",
      "|    iterations         | 4000        |\n",
      "|    time_elapsed       | 27          |\n",
      "|    total_timesteps    | 20000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.24       |\n",
      "|    explained_variance | -0.0381     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3999        |\n",
      "|    policy_loss        | 11.6        |\n",
      "|    reward             | 6.217467    |\n",
      "|    reward_max         | 6.217467    |\n",
      "|    reward_mean        | -0.28956848 |\n",
      "|    reward_min         | -7.1542535  |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 10.1        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 721         |\n",
      "|    iterations         | 4100        |\n",
      "|    time_elapsed       | 28          |\n",
      "|    total_timesteps    | 20500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.25       |\n",
      "|    explained_variance | -0.0422     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4099        |\n",
      "|    policy_loss        | -26.9       |\n",
      "|    reward             | 1.3723361   |\n",
      "|    reward_max         | 1.3723361   |\n",
      "|    reward_mean        | 0.3392189   |\n",
      "|    reward_min         | -0.87801576 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 10.9        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 719        |\n",
      "|    iterations         | 4200       |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 21000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.24      |\n",
      "|    explained_variance | -0.018     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4199       |\n",
      "|    policy_loss        | -25.5      |\n",
      "|    reward             | 1.6336954  |\n",
      "|    reward_max         | 3.082636   |\n",
      "|    reward_mean        | 1.2015126  |\n",
      "|    reward_min         | -0.3995748 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 16.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 718        |\n",
      "|    iterations         | 4300       |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 21500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.23      |\n",
      "|    explained_variance | -0.00153   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4299       |\n",
      "|    policy_loss        | 24         |\n",
      "|    reward             | 4.399092   |\n",
      "|    reward_max         | 4.399092   |\n",
      "|    reward_mean        | 1.7591851  |\n",
      "|    reward_min         | -1.8575937 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 15.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 718       |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.24     |\n",
      "|    explained_variance | 0.00072   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | 195       |\n",
      "|    reward             | -7.29735  |\n",
      "|    reward_max         | 9.075525  |\n",
      "|    reward_mean        | -4.169189 |\n",
      "|    reward_min         | -8.945572 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 730       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 718         |\n",
      "|    iterations         | 4500        |\n",
      "|    time_elapsed       | 31          |\n",
      "|    total_timesteps    | 22500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.24       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4499        |\n",
      "|    policy_loss        | -9.48       |\n",
      "|    reward             | -1.3649963  |\n",
      "|    reward_max         | 1.3828297   |\n",
      "|    reward_mean        | -0.15695971 |\n",
      "|    reward_min         | -1.3649963  |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 3.7         |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 714          |\n",
      "|    iterations         | 4600         |\n",
      "|    time_elapsed       | 32           |\n",
      "|    total_timesteps    | 23000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.24        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4599         |\n",
      "|    policy_loss        | 97.9         |\n",
      "|    reward             | 0.9507558    |\n",
      "|    reward_max         | 1.4961978    |\n",
      "|    reward_mean        | -0.011977035 |\n",
      "|    reward_min         | -1.5684385   |\n",
      "|    std                | 1.03         |\n",
      "|    value_loss         | 186          |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 711       |\n",
      "|    iterations         | 4700      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 23500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.25     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4699      |\n",
      "|    policy_loss        | -42.1     |\n",
      "|    reward             | 6.4976745 |\n",
      "|    reward_max         | 8.944963  |\n",
      "|    reward_mean        | 4.061064  |\n",
      "|    reward_min         | -2.125667 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 45.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 711        |\n",
      "|    iterations         | 4800       |\n",
      "|    time_elapsed       | 33         |\n",
      "|    total_timesteps    | 24000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.24      |\n",
      "|    explained_variance | -0.00255   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4799       |\n",
      "|    policy_loss        | 141        |\n",
      "|    reward             | -4.8640833 |\n",
      "|    reward_max         | 7.5768285  |\n",
      "|    reward_mean        | 0.6396387  |\n",
      "|    reward_min         | -4.8721194 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 409        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 710        |\n",
      "|    iterations         | 4900       |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 24500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.23      |\n",
      "|    explained_variance | -20        |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4899       |\n",
      "|    policy_loss        | 11.1       |\n",
      "|    reward             | 1.4045252  |\n",
      "|    reward_max         | 1.4045252  |\n",
      "|    reward_mean        | 0.35985935 |\n",
      "|    reward_min         | -0.479525  |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 3.17       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 707         |\n",
      "|    iterations         | 5000        |\n",
      "|    time_elapsed       | 35          |\n",
      "|    total_timesteps    | 25000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.24       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4999        |\n",
      "|    policy_loss        | -0.277      |\n",
      "|    reward             | 0.79366213  |\n",
      "|    reward_max         | 0.79366213  |\n",
      "|    reward_mean        | -0.39014074 |\n",
      "|    reward_min         | -1.4980184  |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 1.37        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 702         |\n",
      "|    iterations         | 5100        |\n",
      "|    time_elapsed       | 36          |\n",
      "|    total_timesteps    | 25500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.24       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5099        |\n",
      "|    policy_loss        | -62.6       |\n",
      "|    reward             | 4.7730536   |\n",
      "|    reward_max         | 5.129651    |\n",
      "|    reward_mean        | -0.25483528 |\n",
      "|    reward_min         | -4.1583123  |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 101         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 698        |\n",
      "|    iterations         | 5200       |\n",
      "|    time_elapsed       | 37         |\n",
      "|    total_timesteps    | 26000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.24      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5199       |\n",
      "|    policy_loss        | 10.2       |\n",
      "|    reward             | 3.175625   |\n",
      "|    reward_max         | 3.4947429  |\n",
      "|    reward_mean        | 1.3922707  |\n",
      "|    reward_min         | -2.0542283 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 13.6       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 695         |\n",
      "|    iterations         | 5300        |\n",
      "|    time_elapsed       | 38          |\n",
      "|    total_timesteps    | 26500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.22       |\n",
      "|    explained_variance | -13         |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5299        |\n",
      "|    policy_loss        | -4.69       |\n",
      "|    reward             | 0.9385958   |\n",
      "|    reward_max         | 0.9385958   |\n",
      "|    reward_mean        | 0.10461044  |\n",
      "|    reward_min         | -0.57905674 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 1.57        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 687        |\n",
      "|    iterations         | 5400       |\n",
      "|    time_elapsed       | 39         |\n",
      "|    total_timesteps    | 27000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.22      |\n",
      "|    explained_variance | -0.0976    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5399       |\n",
      "|    policy_loss        | -4.92      |\n",
      "|    reward             | -3.8130465 |\n",
      "|    reward_max         | 1.6356177  |\n",
      "|    reward_mean        | -0.9684105 |\n",
      "|    reward_min         | -3.8130465 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 3.63       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 686        |\n",
      "|    iterations         | 5500       |\n",
      "|    time_elapsed       | 40         |\n",
      "|    total_timesteps    | 27500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.25      |\n",
      "|    explained_variance | -2.8e-05   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5499       |\n",
      "|    policy_loss        | -62.2      |\n",
      "|    reward             | 3.4198902  |\n",
      "|    reward_max         | 3.4198902  |\n",
      "|    reward_mean        | -0.8540615 |\n",
      "|    reward_min         | -5.1715593 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 82.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 686        |\n",
      "|    iterations         | 5600       |\n",
      "|    time_elapsed       | 40         |\n",
      "|    total_timesteps    | 28000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.25      |\n",
      "|    explained_variance | 0.123      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5599       |\n",
      "|    policy_loss        | -83.6      |\n",
      "|    reward             | -0.3328029 |\n",
      "|    reward_max         | 19.77628   |\n",
      "|    reward_mean        | 6.217215   |\n",
      "|    reward_min         | -2.3307202 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 161        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 687         |\n",
      "|    iterations         | 5700        |\n",
      "|    time_elapsed       | 41          |\n",
      "|    total_timesteps    | 28500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.25       |\n",
      "|    explained_variance | 6.04e-05    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5699        |\n",
      "|    policy_loss        | 19.8        |\n",
      "|    reward             | 0.84224236  |\n",
      "|    reward_max         | 2.370653    |\n",
      "|    reward_mean        | 0.7682016   |\n",
      "|    reward_min         | 0.033790756 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 6.97        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 688         |\n",
      "|    iterations         | 5800        |\n",
      "|    time_elapsed       | 42          |\n",
      "|    total_timesteps    | 29000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.26       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5799        |\n",
      "|    policy_loss        | 8.52        |\n",
      "|    reward             | 3.9414272   |\n",
      "|    reward_max         | 4.4069357   |\n",
      "|    reward_mean        | 2.2078774   |\n",
      "|    reward_min         | -0.67936283 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 1.93        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 689         |\n",
      "|    iterations         | 5900        |\n",
      "|    time_elapsed       | 42          |\n",
      "|    total_timesteps    | 29500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.28       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5899        |\n",
      "|    policy_loss        | 66.3        |\n",
      "|    reward             | -3.0265048  |\n",
      "|    reward_max         | 3.6309452   |\n",
      "|    reward_mean        | -0.28069764 |\n",
      "|    reward_min         | -3.0265048  |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 108         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 690        |\n",
      "|    iterations         | 6000       |\n",
      "|    time_elapsed       | 43         |\n",
      "|    total_timesteps    | 30000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.29      |\n",
      "|    explained_variance | 0.0373     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5999       |\n",
      "|    policy_loss        | 8.62       |\n",
      "|    reward             | -0.8127021 |\n",
      "|    reward_max         | 6.2995725  |\n",
      "|    reward_mean        | 1.0678583  |\n",
      "|    reward_min         | -2.29427   |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 8.13       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 691          |\n",
      "|    iterations         | 6100         |\n",
      "|    time_elapsed       | 44           |\n",
      "|    total_timesteps    | 30500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.3         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6099         |\n",
      "|    policy_loss        | -7.43        |\n",
      "|    reward             | 0.016044134  |\n",
      "|    reward_max         | 0.28513157   |\n",
      "|    reward_mean        | 0.0071658483 |\n",
      "|    reward_min         | -0.67198056  |\n",
      "|    std                | 1.04         |\n",
      "|    value_loss         | 1.38         |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 692         |\n",
      "|    iterations         | 6200        |\n",
      "|    time_elapsed       | 44          |\n",
      "|    total_timesteps    | 31000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.3        |\n",
      "|    explained_variance | -0.0454     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6199        |\n",
      "|    policy_loss        | 28.7        |\n",
      "|    reward             | 0.43398008  |\n",
      "|    reward_max         | 0.6505135   |\n",
      "|    reward_mean        | 0.17654464  |\n",
      "|    reward_min         | -0.54434747 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 15.3        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 692        |\n",
      "|    iterations         | 6300       |\n",
      "|    time_elapsed       | 45         |\n",
      "|    total_timesteps    | 31500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.29      |\n",
      "|    explained_variance | -0.00239   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6299       |\n",
      "|    policy_loss        | 9.65       |\n",
      "|    reward             | 0.25946492 |\n",
      "|    reward_max         | 5.671764   |\n",
      "|    reward_mean        | 0.8169563  |\n",
      "|    reward_min         | -1.3130598 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 5.58       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 692         |\n",
      "|    iterations         | 6400        |\n",
      "|    time_elapsed       | 46          |\n",
      "|    total_timesteps    | 32000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.28       |\n",
      "|    explained_variance | -0.0263     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6399        |\n",
      "|    policy_loss        | 20.9        |\n",
      "|    reward             | -0.15617755 |\n",
      "|    reward_max         | 1.7148433   |\n",
      "|    reward_mean        | -0.7325719  |\n",
      "|    reward_min         | -4.8968964  |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 15.3        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 693         |\n",
      "|    iterations         | 6500        |\n",
      "|    time_elapsed       | 46          |\n",
      "|    total_timesteps    | 32500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.3        |\n",
      "|    explained_variance | 0.023       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6499        |\n",
      "|    policy_loss        | -0.138      |\n",
      "|    reward             | -0.43625954 |\n",
      "|    reward_max         | 0.22915196  |\n",
      "|    reward_mean        | -0.3356695  |\n",
      "|    reward_min         | -0.95633614 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 0.27        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 693        |\n",
      "|    iterations         | 6600       |\n",
      "|    time_elapsed       | 47         |\n",
      "|    total_timesteps    | 33000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.29      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6599       |\n",
      "|    policy_loss        | 12.7       |\n",
      "|    reward             | 2.8558056  |\n",
      "|    reward_max         | 2.8558056  |\n",
      "|    reward_mean        | 0.96986353 |\n",
      "|    reward_min         | -0.8324641 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 6.72       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 693        |\n",
      "|    iterations         | 6700       |\n",
      "|    time_elapsed       | 48         |\n",
      "|    total_timesteps    | 33500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.31      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6699       |\n",
      "|    policy_loss        | 19.8       |\n",
      "|    reward             | -2.1610532 |\n",
      "|    reward_max         | 3.6516552  |\n",
      "|    reward_mean        | 1.0542668  |\n",
      "|    reward_min         | -2.1610532 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 12.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 693        |\n",
      "|    iterations         | 6800       |\n",
      "|    time_elapsed       | 49         |\n",
      "|    total_timesteps    | 34000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.31      |\n",
      "|    explained_variance | 0.0724     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6799       |\n",
      "|    policy_loss        | 40.4       |\n",
      "|    reward             | -1.7688507 |\n",
      "|    reward_max         | 0.60159385 |\n",
      "|    reward_mean        | -1.3847431 |\n",
      "|    reward_min         | -3.377781  |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 30.7       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 694          |\n",
      "|    iterations         | 6900         |\n",
      "|    time_elapsed       | 49           |\n",
      "|    total_timesteps    | 34500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.32        |\n",
      "|    explained_variance | 0.0224       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6899         |\n",
      "|    policy_loss        | 11.3         |\n",
      "|    reward             | 0.34853333   |\n",
      "|    reward_max         | 0.34853333   |\n",
      "|    reward_mean        | -0.120872065 |\n",
      "|    reward_min         | -0.95326746  |\n",
      "|    std                | 1.05         |\n",
      "|    value_loss         | 2.17         |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 695         |\n",
      "|    iterations         | 7000        |\n",
      "|    time_elapsed       | 50          |\n",
      "|    total_timesteps    | 35000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.32       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6999        |\n",
      "|    policy_loss        | 2.42        |\n",
      "|    reward             | -0.24927339 |\n",
      "|    reward_max         | 1.2460579   |\n",
      "|    reward_mean        | 0.046033412 |\n",
      "|    reward_min         | -1.149094   |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.473       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 695         |\n",
      "|    iterations         | 7100        |\n",
      "|    time_elapsed       | 51          |\n",
      "|    total_timesteps    | 35500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.29       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7099        |\n",
      "|    policy_loss        | -4.03       |\n",
      "|    reward             | -2.3119283  |\n",
      "|    reward_max         | 4.9653378   |\n",
      "|    reward_mean        | -0.20540749 |\n",
      "|    reward_min         | -4.609545   |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 0.87        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 696        |\n",
      "|    iterations         | 7200       |\n",
      "|    time_elapsed       | 51         |\n",
      "|    total_timesteps    | 36000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.29      |\n",
      "|    explained_variance | -0.0192    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7199       |\n",
      "|    policy_loss        | -2.81      |\n",
      "|    reward             | -1.2365149 |\n",
      "|    reward_max         | 0.9430967  |\n",
      "|    reward_mean        | -1.3770301 |\n",
      "|    reward_min         | -4.7410316 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 5.99       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 696        |\n",
      "|    iterations         | 7300       |\n",
      "|    time_elapsed       | 52         |\n",
      "|    total_timesteps    | 36500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.3       |\n",
      "|    explained_variance | 0.0276     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7299       |\n",
      "|    policy_loss        | 7.45       |\n",
      "|    reward             | 0.33916664 |\n",
      "|    reward_max         | 1.579614   |\n",
      "|    reward_mean        | 0.5464171  |\n",
      "|    reward_min         | 0.09588583 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 1.36       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 696         |\n",
      "|    iterations         | 7400        |\n",
      "|    time_elapsed       | 53          |\n",
      "|    total_timesteps    | 37000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.32       |\n",
      "|    explained_variance | -0.0459     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7399        |\n",
      "|    policy_loss        | 4.07        |\n",
      "|    reward             | -0.58360094 |\n",
      "|    reward_max         | 2.1980462   |\n",
      "|    reward_mean        | 0.5550864   |\n",
      "|    reward_min         | -0.58360094 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 4.85        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 697        |\n",
      "|    iterations         | 7500       |\n",
      "|    time_elapsed       | 53         |\n",
      "|    total_timesteps    | 37500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.34      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7499       |\n",
      "|    policy_loss        | 17.3       |\n",
      "|    reward             | -0.5609929 |\n",
      "|    reward_max         | 1.7648604  |\n",
      "|    reward_mean        | 0.83085585 |\n",
      "|    reward_min         | -0.5609929 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 16.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 697        |\n",
      "|    iterations         | 7600       |\n",
      "|    time_elapsed       | 54         |\n",
      "|    total_timesteps    | 38000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.35      |\n",
      "|    explained_variance | 0.00667    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7599       |\n",
      "|    policy_loss        | 28         |\n",
      "|    reward             | -6.820671  |\n",
      "|    reward_max         | 7.170813   |\n",
      "|    reward_mean        | 0.35739994 |\n",
      "|    reward_min         | -6.820671  |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 24.5       |\n",
      "--------------------------------------\n",
      "day: 2011, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3561940.55\n",
      "total_reward: 2561940.55\n",
      "total_cost: 42619.30\n",
      "total_trades: 7892\n",
      "Sharpe: 0.853\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 697          |\n",
      "|    iterations         | 7700         |\n",
      "|    time_elapsed       | 55           |\n",
      "|    total_timesteps    | 38500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.36        |\n",
      "|    explained_variance | -0.105       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7699         |\n",
      "|    policy_loss        | 4.21         |\n",
      "|    reward             | -0.8219564   |\n",
      "|    reward_max         | 1.1147842    |\n",
      "|    reward_mean        | -0.014251438 |\n",
      "|    reward_min         | -0.8219564   |\n",
      "|    std                | 1.05         |\n",
      "|    value_loss         | 0.392        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 697         |\n",
      "|    iterations         | 7800        |\n",
      "|    time_elapsed       | 55          |\n",
      "|    total_timesteps    | 39000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.35       |\n",
      "|    explained_variance | 0.0336      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7799        |\n",
      "|    policy_loss        | 2.45        |\n",
      "|    reward             | -0.37057123 |\n",
      "|    reward_max         | 1.4172739   |\n",
      "|    reward_mean        | 0.008495671 |\n",
      "|    reward_min         | -2.119485   |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 4.26        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 698        |\n",
      "|    iterations         | 7900       |\n",
      "|    time_elapsed       | 56         |\n",
      "|    total_timesteps    | 39500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.35      |\n",
      "|    explained_variance | -0.0378    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7899       |\n",
      "|    policy_loss        | -25.6      |\n",
      "|    reward             | -1.5471852 |\n",
      "|    reward_max         | 5.161512   |\n",
      "|    reward_mean        | 0.5739994  |\n",
      "|    reward_min         | -1.5471852 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 13.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 698        |\n",
      "|    iterations         | 8000       |\n",
      "|    time_elapsed       | 57         |\n",
      "|    total_timesteps    | 40000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.35      |\n",
      "|    explained_variance | 0.0142     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7999       |\n",
      "|    policy_loss        | 4.58       |\n",
      "|    reward             | -6.54274   |\n",
      "|    reward_max         | 6.792078   |\n",
      "|    reward_mean        | 0.72238487 |\n",
      "|    reward_min         | -6.54274   |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 37.4       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 698          |\n",
      "|    iterations         | 8100         |\n",
      "|    time_elapsed       | 57           |\n",
      "|    total_timesteps    | 40500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.38        |\n",
      "|    explained_variance | -0.308       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8099         |\n",
      "|    policy_loss        | 2.25         |\n",
      "|    reward             | 0.20328026   |\n",
      "|    reward_max         | 0.26320836   |\n",
      "|    reward_mean        | -0.070461884 |\n",
      "|    reward_min         | -0.58983123  |\n",
      "|    std                | 1.06         |\n",
      "|    value_loss         | 0.243        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 699         |\n",
      "|    iterations         | 8200        |\n",
      "|    time_elapsed       | 58          |\n",
      "|    total_timesteps    | 41000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.4        |\n",
      "|    explained_variance | 0.461       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8199        |\n",
      "|    policy_loss        | -39.6       |\n",
      "|    reward             | -0.17790678 |\n",
      "|    reward_max         | 7.2036724   |\n",
      "|    reward_mean        | 2.0055482   |\n",
      "|    reward_min         | -0.17790678 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 26.5        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 699        |\n",
      "|    iterations         | 8300       |\n",
      "|    time_elapsed       | 59         |\n",
      "|    total_timesteps    | 41500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.4       |\n",
      "|    explained_variance | -0.262     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8299       |\n",
      "|    policy_loss        | 22.7       |\n",
      "|    reward             | 0.4097562  |\n",
      "|    reward_max         | 1.5818535  |\n",
      "|    reward_mean        | -1.5830743 |\n",
      "|    reward_min         | -6.724192  |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 11.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 699        |\n",
      "|    iterations         | 8400       |\n",
      "|    time_elapsed       | 60         |\n",
      "|    total_timesteps    | 42000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.4       |\n",
      "|    explained_variance | -0.0115    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8399       |\n",
      "|    policy_loss        | -7.12      |\n",
      "|    reward             | 7.3073235  |\n",
      "|    reward_max         | 7.3073235  |\n",
      "|    reward_mean        | -0.6555985 |\n",
      "|    reward_min         | -7.2021904 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 48.1       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 700         |\n",
      "|    iterations         | 8500        |\n",
      "|    time_elapsed       | 60          |\n",
      "|    total_timesteps    | 42500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.4        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8499        |\n",
      "|    policy_loss        | 2           |\n",
      "|    reward             | 0.11495475  |\n",
      "|    reward_max         | 0.11495475  |\n",
      "|    reward_mean        | -0.07043464 |\n",
      "|    reward_min         | -0.36513874 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.439       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 700        |\n",
      "|    iterations         | 8600       |\n",
      "|    time_elapsed       | 61         |\n",
      "|    total_timesteps    | 43000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.39      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8599       |\n",
      "|    policy_loss        | -20.6      |\n",
      "|    reward             | -2.439607  |\n",
      "|    reward_max         | 0.93781656 |\n",
      "|    reward_mean        | -1.9659777 |\n",
      "|    reward_min         | -3.4120932 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 8.9        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 701        |\n",
      "|    iterations         | 8700       |\n",
      "|    time_elapsed       | 62         |\n",
      "|    total_timesteps    | 43500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.41      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8699       |\n",
      "|    policy_loss        | 0.287      |\n",
      "|    reward             | -1.9933374 |\n",
      "|    reward_max         | 3.2414274  |\n",
      "|    reward_mean        | 0.09328754 |\n",
      "|    reward_min         | -2.2896085 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 2.7        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 701         |\n",
      "|    iterations         | 8800        |\n",
      "|    time_elapsed       | 62          |\n",
      "|    total_timesteps    | 44000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.41       |\n",
      "|    explained_variance | 0.0305      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8799        |\n",
      "|    policy_loss        | -14.5       |\n",
      "|    reward             | 5.5583205   |\n",
      "|    reward_max         | 5.5583205   |\n",
      "|    reward_mean        | 0.009087548 |\n",
      "|    reward_min         | -4.296586   |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 17.9        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 701        |\n",
      "|    iterations         | 8900       |\n",
      "|    time_elapsed       | 63         |\n",
      "|    total_timesteps    | 44500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.41      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8899       |\n",
      "|    policy_loss        | -9.61      |\n",
      "|    reward             | -0.7207819 |\n",
      "|    reward_max         | 3.0275004  |\n",
      "|    reward_mean        | 0.4718358  |\n",
      "|    reward_min         | -0.7449249 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 2.27       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 702        |\n",
      "|    iterations         | 9000       |\n",
      "|    time_elapsed       | 64         |\n",
      "|    total_timesteps    | 45000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.42      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8999       |\n",
      "|    policy_loss        | 3.65       |\n",
      "|    reward             | -6.7189374 |\n",
      "|    reward_max         | 3.2836535  |\n",
      "|    reward_mean        | 0.1476501  |\n",
      "|    reward_min         | -6.7189374 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 2.89       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 702        |\n",
      "|    iterations         | 9100       |\n",
      "|    time_elapsed       | 64         |\n",
      "|    total_timesteps    | 45500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.46      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9099       |\n",
      "|    policy_loss        | -24.9      |\n",
      "|    reward             | -6.0939293 |\n",
      "|    reward_max         | 4.470501   |\n",
      "|    reward_mean        | 0.6226336  |\n",
      "|    reward_min         | -6.0939293 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 22.7       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 702          |\n",
      "|    iterations         | 9200         |\n",
      "|    time_elapsed       | 65           |\n",
      "|    total_timesteps    | 46000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.47        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9199         |\n",
      "|    policy_loss        | 10.5         |\n",
      "|    reward             | 0.6276561    |\n",
      "|    reward_max         | 3.1706808    |\n",
      "|    reward_mean        | 1.38709      |\n",
      "|    reward_min         | -0.056930486 |\n",
      "|    std                | 1.08         |\n",
      "|    value_loss         | 16.9         |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 701         |\n",
      "|    iterations         | 9300        |\n",
      "|    time_elapsed       | 66          |\n",
      "|    total_timesteps    | 46500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.51       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9299        |\n",
      "|    policy_loss        | -5.35       |\n",
      "|    reward             | 0.30831966  |\n",
      "|    reward_max         | 0.7915525   |\n",
      "|    reward_mean        | 0.2331527   |\n",
      "|    reward_min         | -0.72429717 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 0.629       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 701         |\n",
      "|    iterations         | 9400        |\n",
      "|    time_elapsed       | 67          |\n",
      "|    total_timesteps    | 47000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.52       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9399        |\n",
      "|    policy_loss        | 34.1        |\n",
      "|    reward             | 2.519113    |\n",
      "|    reward_max         | 2.519113    |\n",
      "|    reward_mean        | -0.75482345 |\n",
      "|    reward_min         | -3.285772   |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 25.3        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 701         |\n",
      "|    iterations         | 9500        |\n",
      "|    time_elapsed       | 67          |\n",
      "|    total_timesteps    | 47500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.49       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9499        |\n",
      "|    policy_loss        | 112         |\n",
      "|    reward             | -0.29747796 |\n",
      "|    reward_max         | 9.849156    |\n",
      "|    reward_mean        | 3.8664145   |\n",
      "|    reward_min         | -0.29747796 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 232         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 701        |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 68         |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.51      |\n",
      "|    explained_variance | -0.00318   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | 72.2       |\n",
      "|    reward             | 4.375022   |\n",
      "|    reward_max         | 6.211283   |\n",
      "|    reward_mean        | 0.87499714 |\n",
      "|    reward_min         | -5.695929  |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 91.2       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 701         |\n",
      "|    iterations         | 9700        |\n",
      "|    time_elapsed       | 69          |\n",
      "|    total_timesteps    | 48500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.52       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9699        |\n",
      "|    policy_loss        | -3.25       |\n",
      "|    reward             | -0.21269423 |\n",
      "|    reward_max         | 0.5348669   |\n",
      "|    reward_mean        | -0.1601313  |\n",
      "|    reward_min         | -0.64453185 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 1           |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 702        |\n",
      "|    iterations         | 9800       |\n",
      "|    time_elapsed       | 69         |\n",
      "|    total_timesteps    | 49000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.53      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9799       |\n",
      "|    policy_loss        | -33        |\n",
      "|    reward             | 1.4925241  |\n",
      "|    reward_max         | 5.045044   |\n",
      "|    reward_mean        | -0.9268206 |\n",
      "|    reward_min         | -7.0157113 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 28.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 702        |\n",
      "|    iterations         | 9900       |\n",
      "|    time_elapsed       | 70         |\n",
      "|    total_timesteps    | 49500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.54      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9899       |\n",
      "|    policy_loss        | -40        |\n",
      "|    reward             | -7.504576  |\n",
      "|    reward_max         | 3.5146337  |\n",
      "|    reward_mean        | -0.6759211 |\n",
      "|    reward_min         | -7.504576  |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 25.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 702        |\n",
      "|    iterations         | 10000      |\n",
      "|    time_elapsed       | 71         |\n",
      "|    total_timesteps    | 50000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.54      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9999       |\n",
      "|    policy_loss        | 36.7       |\n",
      "|    reward             | -1.7541889 |\n",
      "|    reward_max         | 6.4101853  |\n",
      "|    reward_mean        | 0.22424217 |\n",
      "|    reward_min         | -5.4831886 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 39.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 702        |\n",
      "|    iterations         | 10100      |\n",
      "|    time_elapsed       | 71         |\n",
      "|    total_timesteps    | 50500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.57      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10099      |\n",
      "|    policy_loss        | -32.6      |\n",
      "|    reward             | 0.23310718 |\n",
      "|    reward_max         | 0.7268051  |\n",
      "|    reward_mean        | 0.25641972 |\n",
      "|    reward_min         | -0.664229  |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 25.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 703        |\n",
      "|    iterations         | 10200      |\n",
      "|    time_elapsed       | 72         |\n",
      "|    total_timesteps    | 51000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.58      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10199      |\n",
      "|    policy_loss        | -19.9      |\n",
      "|    reward             | 4.1435566  |\n",
      "|    reward_max         | 4.1435566  |\n",
      "|    reward_mean        | -1.4162846 |\n",
      "|    reward_min         | -6.9195695 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 10.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 702        |\n",
      "|    iterations         | 10300      |\n",
      "|    time_elapsed       | 73         |\n",
      "|    total_timesteps    | 51500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.59      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10299      |\n",
      "|    policy_loss        | 23.7       |\n",
      "|    reward             | 2.2319865  |\n",
      "|    reward_max         | 6.093328   |\n",
      "|    reward_mean        | 1.3686218  |\n",
      "|    reward_min         | -4.1677637 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 18.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 701       |\n",
      "|    iterations         | 10400     |\n",
      "|    time_elapsed       | 74        |\n",
      "|    total_timesteps    | 52000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.59     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10399     |\n",
      "|    policy_loss        | -31.8     |\n",
      "|    reward             | 9.233105  |\n",
      "|    reward_max         | 9.87106   |\n",
      "|    reward_mean        | 2.869561  |\n",
      "|    reward_min         | -5.384901 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 36.5      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 701         |\n",
      "|    iterations         | 10500       |\n",
      "|    time_elapsed       | 74          |\n",
      "|    total_timesteps    | 52500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.6        |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10499       |\n",
      "|    policy_loss        | 0.0914      |\n",
      "|    reward             | 0.4273248   |\n",
      "|    reward_max         | 0.8833623   |\n",
      "|    reward_mean        | 0.109187946 |\n",
      "|    reward_min         | -0.9951879  |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 0.408       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 701         |\n",
      "|    iterations         | 10600       |\n",
      "|    time_elapsed       | 75          |\n",
      "|    total_timesteps    | 53000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.6        |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10599       |\n",
      "|    policy_loss        | -5.93       |\n",
      "|    reward             | -0.17769796 |\n",
      "|    reward_max         | 1.7425891   |\n",
      "|    reward_mean        | 0.29789215  |\n",
      "|    reward_min         | -0.9950267  |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 1.21        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 701        |\n",
      "|    iterations         | 10700      |\n",
      "|    time_elapsed       | 76         |\n",
      "|    total_timesteps    | 53500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.62      |\n",
      "|    explained_variance | 0.00394    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10699      |\n",
      "|    policy_loss        | 25.9       |\n",
      "|    reward             | 4.086676   |\n",
      "|    reward_max         | 4.086676   |\n",
      "|    reward_mean        | -1.0521928 |\n",
      "|    reward_min         | -4.4013205 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 15.3       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 701         |\n",
      "|    iterations         | 10800       |\n",
      "|    time_elapsed       | 76          |\n",
      "|    total_timesteps    | 54000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.62       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10799       |\n",
      "|    policy_loss        | 50.6        |\n",
      "|    reward             | 0.085672565 |\n",
      "|    reward_max         | 1.5513097   |\n",
      "|    reward_mean        | -4.8418183  |\n",
      "|    reward_min         | -14.712953  |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 54          |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 701         |\n",
      "|    iterations         | 10900       |\n",
      "|    time_elapsed       | 77          |\n",
      "|    total_timesteps    | 54500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.6        |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10899       |\n",
      "|    policy_loss        | -12.6       |\n",
      "|    reward             | 1.3332068   |\n",
      "|    reward_max         | 1.4866287   |\n",
      "|    reward_mean        | -0.21062717 |\n",
      "|    reward_min         | -1.9051528  |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 3.41        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 700        |\n",
      "|    iterations         | 11000      |\n",
      "|    time_elapsed       | 78         |\n",
      "|    total_timesteps    | 55000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.63      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10999      |\n",
      "|    policy_loss        | 1.98       |\n",
      "|    reward             | 0.96165246 |\n",
      "|    reward_max         | 1.826516   |\n",
      "|    reward_mean        | 0.3982855  |\n",
      "|    reward_min         | -1.1660639 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 1.23       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 700         |\n",
      "|    iterations         | 11100       |\n",
      "|    time_elapsed       | 79          |\n",
      "|    total_timesteps    | 55500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.62       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11099       |\n",
      "|    policy_loss        | 65.9        |\n",
      "|    reward             | -10.540663  |\n",
      "|    reward_max         | 5.769573    |\n",
      "|    reward_mean        | -0.52452344 |\n",
      "|    reward_min         | -10.540663  |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 80.9        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 701        |\n",
      "|    iterations         | 11200      |\n",
      "|    time_elapsed       | 79         |\n",
      "|    total_timesteps    | 56000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.62      |\n",
      "|    explained_variance | 0.0371     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11199      |\n",
      "|    policy_loss        | -47.7      |\n",
      "|    reward             | -2.4434562 |\n",
      "|    reward_max         | 3.7017937  |\n",
      "|    reward_mean        | -2.0526605 |\n",
      "|    reward_min         | -10.598148 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 62.9       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 701         |\n",
      "|    iterations         | 11300       |\n",
      "|    time_elapsed       | 80          |\n",
      "|    total_timesteps    | 56500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.64       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11299       |\n",
      "|    policy_loss        | -3          |\n",
      "|    reward             | -0.06415199 |\n",
      "|    reward_max         | 0.5041596   |\n",
      "|    reward_mean        | 0.18179193  |\n",
      "|    reward_min         | -0.08641504 |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 0.333       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 701          |\n",
      "|    iterations         | 11400        |\n",
      "|    time_elapsed       | 81           |\n",
      "|    total_timesteps    | 57000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.66        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 11399        |\n",
      "|    policy_loss        | -9.96        |\n",
      "|    reward             | 0.09703635   |\n",
      "|    reward_max         | 1.5391793    |\n",
      "|    reward_mean        | 0.5694071    |\n",
      "|    reward_min         | -0.052139927 |\n",
      "|    std                | 1.12         |\n",
      "|    value_loss         | 2.93         |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 702         |\n",
      "|    iterations         | 11500       |\n",
      "|    time_elapsed       | 81          |\n",
      "|    total_timesteps    | 57500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.66       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11499       |\n",
      "|    policy_loss        | -4.88       |\n",
      "|    reward             | -0.33009756 |\n",
      "|    reward_max         | 3.4008853   |\n",
      "|    reward_mean        | 0.2858893   |\n",
      "|    reward_min         | -2.1654398  |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 4.6         |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 702         |\n",
      "|    iterations         | 11600       |\n",
      "|    time_elapsed       | 82          |\n",
      "|    total_timesteps    | 58000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.65       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11599       |\n",
      "|    policy_loss        | 13.3        |\n",
      "|    reward             | 4.757327    |\n",
      "|    reward_max         | 6.5690045   |\n",
      "|    reward_mean        | 2.538696    |\n",
      "|    reward_min         | -0.22833078 |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 9.46        |\n",
      "---------------------------------------\n",
      "day: 2011, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3928324.46\n",
      "total_reward: 2928324.46\n",
      "total_cost: 2897.94\n",
      "total_trades: 6869\n",
      "Sharpe: 0.905\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 702        |\n",
      "|    iterations         | 11700      |\n",
      "|    time_elapsed       | 83         |\n",
      "|    total_timesteps    | 58500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.65      |\n",
      "|    explained_variance | -1.13      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11699      |\n",
      "|    policy_loss        | 2.39       |\n",
      "|    reward             | -0.269436  |\n",
      "|    reward_max         | 0.6655542  |\n",
      "|    reward_mean        | 0.13620852 |\n",
      "|    reward_min         | -0.269436  |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 0.242      |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 702          |\n",
      "|    iterations         | 11800        |\n",
      "|    time_elapsed       | 83           |\n",
      "|    total_timesteps    | 59000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.66        |\n",
      "|    explained_variance | 5.96e-08     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 11799        |\n",
      "|    policy_loss        | -13.7        |\n",
      "|    reward             | 0.11207014   |\n",
      "|    reward_max         | 2.3451824    |\n",
      "|    reward_mean        | -0.006843654 |\n",
      "|    reward_min         | -2.088835    |\n",
      "|    std                | 1.12         |\n",
      "|    value_loss         | 7.95         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 702        |\n",
      "|    iterations         | 11900      |\n",
      "|    time_elapsed       | 84         |\n",
      "|    total_timesteps    | 59500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.66      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11899      |\n",
      "|    policy_loss        | -54.5      |\n",
      "|    reward             | 0.67827106 |\n",
      "|    reward_max         | 3.112777   |\n",
      "|    reward_mean        | 0.25949356 |\n",
      "|    reward_min         | -5.0802073 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 50.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 703        |\n",
      "|    iterations         | 12000      |\n",
      "|    time_elapsed       | 85         |\n",
      "|    total_timesteps    | 60000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.65      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11999      |\n",
      "|    policy_loss        | 53.9       |\n",
      "|    reward             | 7.535649   |\n",
      "|    reward_max         | 7.535649   |\n",
      "|    reward_mean        | 1.4922909  |\n",
      "|    reward_min         | -2.0523448 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 69         |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 703         |\n",
      "|    iterations         | 12100       |\n",
      "|    time_elapsed       | 86          |\n",
      "|    total_timesteps    | 60500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.67       |\n",
      "|    explained_variance | 0.00572     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12099       |\n",
      "|    policy_loss        | -3.09       |\n",
      "|    reward             | 0.019666433 |\n",
      "|    reward_max         | 1.83727     |\n",
      "|    reward_mean        | -0.12972504 |\n",
      "|    reward_min         | -1.557909   |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 0.24        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 703         |\n",
      "|    iterations         | 12200       |\n",
      "|    time_elapsed       | 86          |\n",
      "|    total_timesteps    | 61000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.69       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12199       |\n",
      "|    policy_loss        | 17.8        |\n",
      "|    reward             | -1.450441   |\n",
      "|    reward_max         | 1.941967    |\n",
      "|    reward_mean        | -0.07901972 |\n",
      "|    reward_min         | -1.450441   |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 5.78        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 703        |\n",
      "|    iterations         | 12300      |\n",
      "|    time_elapsed       | 87         |\n",
      "|    total_timesteps    | 61500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.68      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12299      |\n",
      "|    policy_loss        | 11.7       |\n",
      "|    reward             | 3.7060354  |\n",
      "|    reward_max         | 4.9049454  |\n",
      "|    reward_mean        | 1.004174   |\n",
      "|    reward_min         | -3.7338893 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 4.45       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 702         |\n",
      "|    iterations         | 12400       |\n",
      "|    time_elapsed       | 88          |\n",
      "|    total_timesteps    | 62000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.68       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12399       |\n",
      "|    policy_loss        | -13.5       |\n",
      "|    reward             | -1.1073115  |\n",
      "|    reward_max         | 1.857879    |\n",
      "|    reward_mean        | -0.35759225 |\n",
      "|    reward_min         | -2.9327185  |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 19.6        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 701         |\n",
      "|    iterations         | 12500       |\n",
      "|    time_elapsed       | 89          |\n",
      "|    total_timesteps    | 62500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.68       |\n",
      "|    explained_variance | 0.122       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12499       |\n",
      "|    policy_loss        | -4.23       |\n",
      "|    reward             | -0.48481143 |\n",
      "|    reward_max         | 1.5727009   |\n",
      "|    reward_mean        | 0.31465107  |\n",
      "|    reward_min         | -0.905461   |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 4.61        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 700        |\n",
      "|    iterations         | 12600      |\n",
      "|    time_elapsed       | 89         |\n",
      "|    total_timesteps    | 63000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.7       |\n",
      "|    explained_variance | 0.19       |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12599      |\n",
      "|    policy_loss        | -19.9      |\n",
      "|    reward             | 0.9569681  |\n",
      "|    reward_max         | 0.9569681  |\n",
      "|    reward_mean        | 0.201703   |\n",
      "|    reward_min         | -1.0834085 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 12.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 700        |\n",
      "|    iterations         | 12700      |\n",
      "|    time_elapsed       | 90         |\n",
      "|    total_timesteps    | 63500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.71      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12699      |\n",
      "|    policy_loss        | -20.6      |\n",
      "|    reward             | -4.930634  |\n",
      "|    reward_max         | 4.57508    |\n",
      "|    reward_mean        | -1.438866  |\n",
      "|    reward_min         | -7.2620997 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 8.2        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 700         |\n",
      "|    iterations         | 12800       |\n",
      "|    time_elapsed       | 91          |\n",
      "|    total_timesteps    | 64000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.71       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12799       |\n",
      "|    policy_loss        | -74.2       |\n",
      "|    reward             | -3.1123278  |\n",
      "|    reward_max         | 2.7519908   |\n",
      "|    reward_mean        | -0.99691087 |\n",
      "|    reward_min         | -6.40549    |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 166         |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 700         |\n",
      "|    iterations         | 12900       |\n",
      "|    time_elapsed       | 92          |\n",
      "|    total_timesteps    | 64500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.69       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12899       |\n",
      "|    policy_loss        | -16.2       |\n",
      "|    reward             | 0.63749164  |\n",
      "|    reward_max         | 0.63749164  |\n",
      "|    reward_mean        | 0.2291914   |\n",
      "|    reward_min         | -0.14482582 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 4.6         |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 700         |\n",
      "|    iterations         | 13000       |\n",
      "|    time_elapsed       | 92          |\n",
      "|    total_timesteps    | 65000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.69       |\n",
      "|    explained_variance | -0.0682     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12999       |\n",
      "|    policy_loss        | 7.54        |\n",
      "|    reward             | -0.18571895 |\n",
      "|    reward_max         | 0.4254041   |\n",
      "|    reward_mean        | -0.07934317 |\n",
      "|    reward_min         | -0.55710584 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 2.38        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 699        |\n",
      "|    iterations         | 13100      |\n",
      "|    time_elapsed       | 93         |\n",
      "|    total_timesteps    | 65500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.69      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13099      |\n",
      "|    policy_loss        | 45.1       |\n",
      "|    reward             | -0.8126179 |\n",
      "|    reward_max         | 6.7154865  |\n",
      "|    reward_mean        | 1.0635116  |\n",
      "|    reward_min         | -3.831323  |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 35.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 700        |\n",
      "|    iterations         | 13200      |\n",
      "|    time_elapsed       | 94         |\n",
      "|    total_timesteps    | 66000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.69      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13199      |\n",
      "|    policy_loss        | 51.1       |\n",
      "|    reward             | -3.8794396 |\n",
      "|    reward_max         | 6.0942717  |\n",
      "|    reward_mean        | 0.6400911  |\n",
      "|    reward_min         | -3.8794396 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 28.1       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 700         |\n",
      "|    iterations         | 13300       |\n",
      "|    time_elapsed       | 94          |\n",
      "|    total_timesteps    | 66500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.72       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13299       |\n",
      "|    policy_loss        | 13          |\n",
      "|    reward             | 0.23894742  |\n",
      "|    reward_max         | 0.3087879   |\n",
      "|    reward_mean        | 0.025753662 |\n",
      "|    reward_min         | -0.44312906 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 3.07        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 700         |\n",
      "|    iterations         | 13400       |\n",
      "|    time_elapsed       | 95          |\n",
      "|    total_timesteps    | 67000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.72       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13399       |\n",
      "|    policy_loss        | 17.1        |\n",
      "|    reward             | -1.948044   |\n",
      "|    reward_max         | 1.0050074   |\n",
      "|    reward_mean        | -0.27559313 |\n",
      "|    reward_min         | -1.948044   |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 5.35        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 700       |\n",
      "|    iterations         | 13500     |\n",
      "|    time_elapsed       | 96        |\n",
      "|    total_timesteps    | 67500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.71     |\n",
      "|    explained_variance | 0.000275  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13499     |\n",
      "|    policy_loss        | -28.4     |\n",
      "|    reward             | 1.2345115 |\n",
      "|    reward_max         | 9.481932  |\n",
      "|    reward_mean        | 2.295338  |\n",
      "|    reward_min         | -3.996119 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 40.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 700        |\n",
      "|    iterations         | 13600      |\n",
      "|    time_elapsed       | 97         |\n",
      "|    total_timesteps    | 68000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.7       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13599      |\n",
      "|    policy_loss        | -56.8      |\n",
      "|    reward             | 7.357841   |\n",
      "|    reward_max         | 7.357841   |\n",
      "|    reward_mean        | 0.61121935 |\n",
      "|    reward_min         | -7.330109  |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 127        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 700         |\n",
      "|    iterations         | 13700       |\n",
      "|    time_elapsed       | 97          |\n",
      "|    total_timesteps    | 68500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.72       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13699       |\n",
      "|    policy_loss        | -9.82       |\n",
      "|    reward             | 1.5120103   |\n",
      "|    reward_max         | 1.7305944   |\n",
      "|    reward_mean        | 0.43182966  |\n",
      "|    reward_min         | -0.88583416 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 1.51        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 700         |\n",
      "|    iterations         | 13800       |\n",
      "|    time_elapsed       | 98          |\n",
      "|    total_timesteps    | 69000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.71       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13799       |\n",
      "|    policy_loss        | -16.9       |\n",
      "|    reward             | 1.7099329   |\n",
      "|    reward_max         | 1.7099329   |\n",
      "|    reward_mean        | 1.020104    |\n",
      "|    reward_min         | -0.20813257 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 6.49        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 700        |\n",
      "|    iterations         | 13900      |\n",
      "|    time_elapsed       | 99         |\n",
      "|    total_timesteps    | 69500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.69      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13899      |\n",
      "|    policy_loss        | 75.5       |\n",
      "|    reward             | -0.347736  |\n",
      "|    reward_max         | 0.98503834 |\n",
      "|    reward_mean        | -1.42274   |\n",
      "|    reward_min         | -6.417204  |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 88.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 700       |\n",
      "|    iterations         | 14000     |\n",
      "|    time_elapsed       | 99        |\n",
      "|    total_timesteps    | 70000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.7      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13999     |\n",
      "|    policy_loss        | -75       |\n",
      "|    reward             | -3.108855 |\n",
      "|    reward_max         | 6.738385  |\n",
      "|    reward_mean        | 0.9080916 |\n",
      "|    reward_min         | -9.457134 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 93.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 699        |\n",
      "|    iterations         | 14100      |\n",
      "|    time_elapsed       | 100        |\n",
      "|    total_timesteps    | 70500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.69      |\n",
      "|    explained_variance | -0.0092    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14099      |\n",
      "|    policy_loss        | -6.08      |\n",
      "|    reward             | -1.2548337 |\n",
      "|    reward_max         | 0.7593306  |\n",
      "|    reward_mean        | -0.5507614 |\n",
      "|    reward_min         | -2.1294851 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 0.753      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 699        |\n",
      "|    iterations         | 14200      |\n",
      "|    time_elapsed       | 101        |\n",
      "|    total_timesteps    | 71000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.7       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14199      |\n",
      "|    policy_loss        | 27.7       |\n",
      "|    reward             | -2.3298671 |\n",
      "|    reward_max         | 1.0202248  |\n",
      "|    reward_mean        | -0.5647086 |\n",
      "|    reward_min         | -2.3298671 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 11.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 699        |\n",
      "|    iterations         | 14300      |\n",
      "|    time_elapsed       | 102        |\n",
      "|    total_timesteps    | 71500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.7       |\n",
      "|    explained_variance | 7.51e-06   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14299      |\n",
      "|    policy_loss        | 21         |\n",
      "|    reward             | -4.0576077 |\n",
      "|    reward_max         | 6.4859214  |\n",
      "|    reward_mean        | 0.60943455 |\n",
      "|    reward_min         | -5.831535  |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 47.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 697        |\n",
      "|    iterations         | 14400      |\n",
      "|    time_elapsed       | 103        |\n",
      "|    total_timesteps    | 72000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.69      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14399      |\n",
      "|    policy_loss        | -32.6      |\n",
      "|    reward             | 1.6677747  |\n",
      "|    reward_max         | 1.6677747  |\n",
      "|    reward_mean        | -1.7605476 |\n",
      "|    reward_min         | -4.1518435 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 20.3       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 697         |\n",
      "|    iterations         | 14500       |\n",
      "|    time_elapsed       | 103         |\n",
      "|    total_timesteps    | 72500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.71       |\n",
      "|    explained_variance | 1.79e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14499       |\n",
      "|    policy_loss        | -7.07       |\n",
      "|    reward             | 0.65496063  |\n",
      "|    reward_max         | 0.9120013   |\n",
      "|    reward_mean        | 0.13719878  |\n",
      "|    reward_min         | -0.99511516 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 0.758       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 697       |\n",
      "|    iterations         | 14600     |\n",
      "|    time_elapsed       | 104       |\n",
      "|    total_timesteps    | 73000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.72     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14599     |\n",
      "|    policy_loss        | -6.05     |\n",
      "|    reward             | -3.732273 |\n",
      "|    reward_max         | 1.8969829 |\n",
      "|    reward_mean        | -0.361005 |\n",
      "|    reward_min         | -3.732273 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 4.4       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 697        |\n",
      "|    iterations         | 14700      |\n",
      "|    time_elapsed       | 105        |\n",
      "|    total_timesteps    | 73500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.73      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14699      |\n",
      "|    policy_loss        | 104        |\n",
      "|    reward             | -5.202772  |\n",
      "|    reward_max         | 9.591276   |\n",
      "|    reward_mean        | 0.22104552 |\n",
      "|    reward_min         | -7.413395  |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 274        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 696        |\n",
      "|    iterations         | 14800      |\n",
      "|    time_elapsed       | 106        |\n",
      "|    total_timesteps    | 74000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.72      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14799      |\n",
      "|    policy_loss        | 72.1       |\n",
      "|    reward             | 3.2313926  |\n",
      "|    reward_max         | 3.3607998  |\n",
      "|    reward_mean        | 0.77552    |\n",
      "|    reward_min         | -3.7166402 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 124        |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 696          |\n",
      "|    iterations         | 14900        |\n",
      "|    time_elapsed       | 106          |\n",
      "|    total_timesteps    | 74500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.72        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 14899        |\n",
      "|    policy_loss        | 9.29         |\n",
      "|    reward             | -0.27757254  |\n",
      "|    reward_max         | 0.36808914   |\n",
      "|    reward_mean        | -0.022505397 |\n",
      "|    reward_min         | -0.43565017  |\n",
      "|    std                | 1.14         |\n",
      "|    value_loss         | 2.04         |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 696         |\n",
      "|    iterations         | 15000       |\n",
      "|    time_elapsed       | 107         |\n",
      "|    total_timesteps    | 75000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.71       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14999       |\n",
      "|    policy_loss        | -3.93       |\n",
      "|    reward             | 0.31046772  |\n",
      "|    reward_max         | 0.73346794  |\n",
      "|    reward_mean        | -0.24939512 |\n",
      "|    reward_min         | -1.8536016  |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 3.02        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 696        |\n",
      "|    iterations         | 15100      |\n",
      "|    time_elapsed       | 108        |\n",
      "|    total_timesteps    | 75500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.71      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15099      |\n",
      "|    policy_loss        | -139       |\n",
      "|    reward             | -19.224113 |\n",
      "|    reward_max         | 18.837841  |\n",
      "|    reward_mean        | -4.1681194 |\n",
      "|    reward_min         | -21.645756 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 276        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 696        |\n",
      "|    iterations         | 15200      |\n",
      "|    time_elapsed       | 109        |\n",
      "|    total_timesteps    | 76000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.71      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15199      |\n",
      "|    policy_loss        | 29.5       |\n",
      "|    reward             | 5.0069404  |\n",
      "|    reward_max         | 5.0069404  |\n",
      "|    reward_mean        | -0.7256084 |\n",
      "|    reward_min         | -7.887599  |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 41.1       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 695         |\n",
      "|    iterations         | 15300       |\n",
      "|    time_elapsed       | 109         |\n",
      "|    total_timesteps    | 76500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.71       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15299       |\n",
      "|    policy_loss        | 9.02        |\n",
      "|    reward             | 0.12867206  |\n",
      "|    reward_max         | 0.38363516  |\n",
      "|    reward_mean        | 0.083057724 |\n",
      "|    reward_min         | -0.3426896  |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 1.33        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 695        |\n",
      "|    iterations         | 15400      |\n",
      "|    time_elapsed       | 110        |\n",
      "|    total_timesteps    | 77000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.71      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15399      |\n",
      "|    policy_loss        | 12.9       |\n",
      "|    reward             | 0.8335357  |\n",
      "|    reward_max         | 1.5773654  |\n",
      "|    reward_mean        | -1.2197362 |\n",
      "|    reward_min         | -3.5214717 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 2.89       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 695        |\n",
      "|    iterations         | 15500      |\n",
      "|    time_elapsed       | 111        |\n",
      "|    total_timesteps    | 77500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.7       |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15499      |\n",
      "|    policy_loss        | -3.47      |\n",
      "|    reward             | -9.77813   |\n",
      "|    reward_max         | 0.05525291 |\n",
      "|    reward_mean        | -5.285889  |\n",
      "|    reward_min         | -9.77813   |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 0.672      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 695        |\n",
      "|    iterations         | 15600      |\n",
      "|    time_elapsed       | 112        |\n",
      "|    total_timesteps    | 78000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.73      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15599      |\n",
      "|    policy_loss        | -79.1      |\n",
      "|    reward             | -3.2106378 |\n",
      "|    reward_max         | 4.205094   |\n",
      "|    reward_mean        | -1.3095208 |\n",
      "|    reward_min         | -6.632535  |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 92.9       |\n",
      "--------------------------------------\n",
      "day: 2011, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3154408.87\n",
      "total_reward: 2154408.87\n",
      "total_cost: 2867.86\n",
      "total_trades: 5878\n",
      "Sharpe: 0.773\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 693         |\n",
      "|    iterations         | 15700       |\n",
      "|    time_elapsed       | 113         |\n",
      "|    total_timesteps    | 78500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.75       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15699       |\n",
      "|    policy_loss        | 0.0216      |\n",
      "|    reward             | 0.1629624   |\n",
      "|    reward_max         | 0.6175436   |\n",
      "|    reward_mean        | 0.40752113  |\n",
      "|    reward_min         | 0.036525562 |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 0.0561      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 692         |\n",
      "|    iterations         | 15800       |\n",
      "|    time_elapsed       | 114         |\n",
      "|    total_timesteps    | 79000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.75       |\n",
      "|    explained_variance | 0.0305      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15799       |\n",
      "|    policy_loss        | -35.7       |\n",
      "|    reward             | -0.24126692 |\n",
      "|    reward_max         | 2.9747508   |\n",
      "|    reward_mean        | -0.5178319  |\n",
      "|    reward_min         | -6.106076   |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 39.2        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 692         |\n",
      "|    iterations         | 15900       |\n",
      "|    time_elapsed       | 114         |\n",
      "|    total_timesteps    | 79500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.74       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15899       |\n",
      "|    policy_loss        | -7.78       |\n",
      "|    reward             | 3.582578    |\n",
      "|    reward_max         | 5.897272    |\n",
      "|    reward_mean        | 2.2456748   |\n",
      "|    reward_min         | -0.79853415 |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 8.19        |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 692          |\n",
      "|    iterations         | 16000        |\n",
      "|    time_elapsed       | 115          |\n",
      "|    total_timesteps    | 80000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.71        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 15999        |\n",
      "|    policy_loss        | -45.8        |\n",
      "|    reward             | 4.0108156    |\n",
      "|    reward_max         | 4.6943727    |\n",
      "|    reward_mean        | 2.6731288    |\n",
      "|    reward_min         | -0.083769426 |\n",
      "|    std                | 1.13         |\n",
      "|    value_loss         | 62.5         |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 691         |\n",
      "|    iterations         | 16100       |\n",
      "|    time_elapsed       | 116         |\n",
      "|    total_timesteps    | 80500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.73       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16099       |\n",
      "|    policy_loss        | 0.457       |\n",
      "|    reward             | -0.7199366  |\n",
      "|    reward_max         | 1.2707949   |\n",
      "|    reward_mean        | 0.054457583 |\n",
      "|    reward_min         | -0.7199366  |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 0.0252      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 692        |\n",
      "|    iterations         | 16200      |\n",
      "|    time_elapsed       | 117        |\n",
      "|    total_timesteps    | 81000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.71      |\n",
      "|    explained_variance | -0.000102  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16199      |\n",
      "|    policy_loss        | 3.72       |\n",
      "|    reward             | 3.250577   |\n",
      "|    reward_max         | 3.250577   |\n",
      "|    reward_mean        | 1.3914156  |\n",
      "|    reward_min         | 0.17276663 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 0.751      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 691        |\n",
      "|    iterations         | 16300      |\n",
      "|    time_elapsed       | 117        |\n",
      "|    total_timesteps    | 81500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.74      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16299      |\n",
      "|    policy_loss        | 12.8       |\n",
      "|    reward             | 0.48605305 |\n",
      "|    reward_max         | 3.3668592  |\n",
      "|    reward_mean        | 0.40166917 |\n",
      "|    reward_min         | -2.0066135 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 4.17       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 691         |\n",
      "|    iterations         | 16400       |\n",
      "|    time_elapsed       | 118         |\n",
      "|    total_timesteps    | 82000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.75       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16399       |\n",
      "|    policy_loss        | -50.5       |\n",
      "|    reward             | 3.0785146   |\n",
      "|    reward_max         | 3.9779267   |\n",
      "|    reward_mean        | -0.34965417 |\n",
      "|    reward_min         | -9.945349   |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 76.5        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 691        |\n",
      "|    iterations         | 16500      |\n",
      "|    time_elapsed       | 119        |\n",
      "|    total_timesteps    | 82500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.77      |\n",
      "|    explained_variance | -1.23      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16499      |\n",
      "|    policy_loss        | -64.8      |\n",
      "|    reward             | 0.20712917 |\n",
      "|    reward_max         | 0.20712917 |\n",
      "|    reward_mean        | 0.00714944 |\n",
      "|    reward_min         | -0.2098741 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 135        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 691         |\n",
      "|    iterations         | 16600       |\n",
      "|    time_elapsed       | 119         |\n",
      "|    total_timesteps    | 83000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.77       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16599       |\n",
      "|    policy_loss        | -1.43       |\n",
      "|    reward             | 0.62304187  |\n",
      "|    reward_max         | 1.7391468   |\n",
      "|    reward_mean        | 0.63890105  |\n",
      "|    reward_min         | -0.38087362 |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 0.137       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 691         |\n",
      "|    iterations         | 16700       |\n",
      "|    time_elapsed       | 120         |\n",
      "|    total_timesteps    | 83500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.77       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16699       |\n",
      "|    policy_loss        | 4.75        |\n",
      "|    reward             | -0.52380687 |\n",
      "|    reward_max         | 4.7368927   |\n",
      "|    reward_mean        | 0.15019931  |\n",
      "|    reward_min         | -2.8483377  |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 0.693       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 691        |\n",
      "|    iterations         | 16800      |\n",
      "|    time_elapsed       | 121        |\n",
      "|    total_timesteps    | 84000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.74      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16799      |\n",
      "|    policy_loss        | -21        |\n",
      "|    reward             | 0.8828344  |\n",
      "|    reward_max         | 7.0659914  |\n",
      "|    reward_mean        | 3.1427581  |\n",
      "|    reward_min         | -0.5586575 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 47.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 691        |\n",
      "|    iterations         | 16900      |\n",
      "|    time_elapsed       | 122        |\n",
      "|    total_timesteps    | 84500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.72      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16899      |\n",
      "|    policy_loss        | 17.7       |\n",
      "|    reward             | 1.7224052  |\n",
      "|    reward_max         | 3.8848395  |\n",
      "|    reward_mean        | 0.7363823  |\n",
      "|    reward_min         | -3.3440099 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 11.3       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 691          |\n",
      "|    iterations         | 17000        |\n",
      "|    time_elapsed       | 123          |\n",
      "|    total_timesteps    | 85000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.73        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 16999        |\n",
      "|    policy_loss        | 13.5         |\n",
      "|    reward             | 0.13544019   |\n",
      "|    reward_max         | 2.3378134    |\n",
      "|    reward_mean        | -0.088833556 |\n",
      "|    reward_min         | -2.0253274   |\n",
      "|    std                | 1.14         |\n",
      "|    value_loss         | 4.11         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 691        |\n",
      "|    iterations         | 17100      |\n",
      "|    time_elapsed       | 123        |\n",
      "|    total_timesteps    | 85500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.75      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17099      |\n",
      "|    policy_loss        | 1.23       |\n",
      "|    reward             | -0.5311382 |\n",
      "|    reward_max         | 2.1821148  |\n",
      "|    reward_mean        | 0.9424881  |\n",
      "|    reward_min         | -0.5311382 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 1.37       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 691        |\n",
      "|    iterations         | 17200      |\n",
      "|    time_elapsed       | 124        |\n",
      "|    total_timesteps    | 86000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.77      |\n",
      "|    explained_variance | -0.00323   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17199      |\n",
      "|    policy_loss        | -61.7      |\n",
      "|    reward             | 10.716431  |\n",
      "|    reward_max         | 10.716431  |\n",
      "|    reward_mean        | 4.2292533  |\n",
      "|    reward_min         | -1.5853838 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 84.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 691        |\n",
      "|    iterations         | 17300      |\n",
      "|    time_elapsed       | 125        |\n",
      "|    total_timesteps    | 86500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.77      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17299      |\n",
      "|    policy_loss        | 6.02       |\n",
      "|    reward             | 1.909736   |\n",
      "|    reward_max         | 2.0437684  |\n",
      "|    reward_mean        | -1.1598338 |\n",
      "|    reward_min         | -4.3755727 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 4.12       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 691         |\n",
      "|    iterations         | 17400       |\n",
      "|    time_elapsed       | 125         |\n",
      "|    total_timesteps    | 87000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.78       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17399       |\n",
      "|    policy_loss        | 10          |\n",
      "|    reward             | -2.3814256  |\n",
      "|    reward_max         | 2.3411732   |\n",
      "|    reward_mean        | 0.082505964 |\n",
      "|    reward_min         | -2.3814256  |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 2.47        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 691        |\n",
      "|    iterations         | 17500      |\n",
      "|    time_elapsed       | 126        |\n",
      "|    total_timesteps    | 87500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.76      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17499      |\n",
      "|    policy_loss        | -20.1      |\n",
      "|    reward             | -1.1508654 |\n",
      "|    reward_max         | 1.7158308  |\n",
      "|    reward_mean        | 0.5487272  |\n",
      "|    reward_min         | -1.1508654 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 7.01       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 691         |\n",
      "|    iterations         | 17600       |\n",
      "|    time_elapsed       | 127         |\n",
      "|    total_timesteps    | 88000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.77       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17599       |\n",
      "|    policy_loss        | 13.9        |\n",
      "|    reward             | -2.5709198  |\n",
      "|    reward_max         | 0.28517973  |\n",
      "|    reward_mean        | -0.99943376 |\n",
      "|    reward_min         | -2.5709198  |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 7.28        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 691        |\n",
      "|    iterations         | 17700      |\n",
      "|    time_elapsed       | 127        |\n",
      "|    total_timesteps    | 88500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.79      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17699      |\n",
      "|    policy_loss        | 25.7       |\n",
      "|    reward             | 6.114961   |\n",
      "|    reward_max         | 6.114961   |\n",
      "|    reward_mean        | 1.9909761  |\n",
      "|    reward_min         | -4.3711596 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 20.7       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 691         |\n",
      "|    iterations         | 17800       |\n",
      "|    time_elapsed       | 128         |\n",
      "|    total_timesteps    | 89000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.77       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17799       |\n",
      "|    policy_loss        | 3.7         |\n",
      "|    reward             | -0.89495987 |\n",
      "|    reward_max         | 0.54844785  |\n",
      "|    reward_mean        | -0.24076262 |\n",
      "|    reward_min         | -1.3088775  |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 0.42        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 691         |\n",
      "|    iterations         | 17900       |\n",
      "|    time_elapsed       | 129         |\n",
      "|    total_timesteps    | 89500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.75       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17899       |\n",
      "|    policy_loss        | 24.1        |\n",
      "|    reward             | 1.4784384   |\n",
      "|    reward_max         | 2.2599318   |\n",
      "|    reward_mean        | 0.5668127   |\n",
      "|    reward_min         | -0.52082527 |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 13.4        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 691         |\n",
      "|    iterations         | 18000       |\n",
      "|    time_elapsed       | 130         |\n",
      "|    total_timesteps    | 90000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.76       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17999       |\n",
      "|    policy_loss        | 89.2        |\n",
      "|    reward             | 1.9309957   |\n",
      "|    reward_max         | 4.276906    |\n",
      "|    reward_mean        | 1.6440712   |\n",
      "|    reward_min         | -0.72696614 |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 147         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 691       |\n",
      "|    iterations         | 18100     |\n",
      "|    time_elapsed       | 130       |\n",
      "|    total_timesteps    | 90500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.76     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18099     |\n",
      "|    policy_loss        | -51.9     |\n",
      "|    reward             | 3.4525342 |\n",
      "|    reward_max         | 6.901878  |\n",
      "|    reward_mean        | 4.296609  |\n",
      "|    reward_min         | 2.0332873 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 54.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 691        |\n",
      "|    iterations         | 18200      |\n",
      "|    time_elapsed       | 131        |\n",
      "|    total_timesteps    | 91000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.77      |\n",
      "|    explained_variance | 7.25e-05   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18199      |\n",
      "|    policy_loss        | 13.1       |\n",
      "|    reward             | -1.0124707 |\n",
      "|    reward_max         | 3.4361928  |\n",
      "|    reward_mean        | 0.45869982 |\n",
      "|    reward_min         | -1.0124707 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 2.46       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 690       |\n",
      "|    iterations         | 18300     |\n",
      "|    time_elapsed       | 132       |\n",
      "|    total_timesteps    | 91500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.8      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18299     |\n",
      "|    policy_loss        | -13.1     |\n",
      "|    reward             | 0.8885164 |\n",
      "|    reward_max         | 3.0483897 |\n",
      "|    reward_mean        | 1.171314  |\n",
      "|    reward_min         | -1.239626 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 3.02      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 690       |\n",
      "|    iterations         | 18400     |\n",
      "|    time_elapsed       | 133       |\n",
      "|    total_timesteps    | 92000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.79     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18399     |\n",
      "|    policy_loss        | 3.6       |\n",
      "|    reward             | 0.8891841 |\n",
      "|    reward_max         | 6.7299366 |\n",
      "|    reward_mean        | 2.652325  |\n",
      "|    reward_min         | 0.8891841 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 3.15      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 690         |\n",
      "|    iterations         | 18500       |\n",
      "|    time_elapsed       | 133         |\n",
      "|    total_timesteps    | 92500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.79       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18499       |\n",
      "|    policy_loss        | 45.5        |\n",
      "|    reward             | -0.47200865 |\n",
      "|    reward_max         | 5.2675996   |\n",
      "|    reward_mean        | 0.4989019   |\n",
      "|    reward_min         | -2.9567528  |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 64.1        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 690        |\n",
      "|    iterations         | 18600      |\n",
      "|    time_elapsed       | 134        |\n",
      "|    total_timesteps    | 93000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.79      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18599      |\n",
      "|    policy_loss        | 7.77       |\n",
      "|    reward             | 0.5182235  |\n",
      "|    reward_max         | 0.5182235  |\n",
      "|    reward_mean        | 0.12151761 |\n",
      "|    reward_min         | -0.270901  |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 1.33       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 691        |\n",
      "|    iterations         | 18700      |\n",
      "|    time_elapsed       | 135        |\n",
      "|    total_timesteps    | 93500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.79      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18699      |\n",
      "|    policy_loss        | -75.7      |\n",
      "|    reward             | 2.6399653  |\n",
      "|    reward_max         | 2.703751   |\n",
      "|    reward_mean        | 0.38738495 |\n",
      "|    reward_min         | -4.4301004 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 65.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 691        |\n",
      "|    iterations         | 18800      |\n",
      "|    time_elapsed       | 135        |\n",
      "|    total_timesteps    | 94000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.78      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18799      |\n",
      "|    policy_loss        | -13.8      |\n",
      "|    reward             | -5.165408  |\n",
      "|    reward_max         | 7.2451396  |\n",
      "|    reward_mean        | -2.1094816 |\n",
      "|    reward_min         | -9.253534  |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 11.9       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 691         |\n",
      "|    iterations         | 18900       |\n",
      "|    time_elapsed       | 136         |\n",
      "|    total_timesteps    | 94500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.79       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18899       |\n",
      "|    policy_loss        | -79.7       |\n",
      "|    reward             | -0.19007123 |\n",
      "|    reward_max         | 1.6407697   |\n",
      "|    reward_mean        | -0.7954802  |\n",
      "|    reward_min         | -6.675183   |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 119         |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 691         |\n",
      "|    iterations         | 19000       |\n",
      "|    time_elapsed       | 137         |\n",
      "|    total_timesteps    | 95000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.81       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18999       |\n",
      "|    policy_loss        | -3.21       |\n",
      "|    reward             | -0.29089576 |\n",
      "|    reward_max         | 0.96698123  |\n",
      "|    reward_mean        | 0.13488628  |\n",
      "|    reward_min         | -1.0474671  |\n",
      "|    std                | 1.16        |\n",
      "|    value_loss         | 0.43        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 691        |\n",
      "|    iterations         | 19100      |\n",
      "|    time_elapsed       | 138        |\n",
      "|    total_timesteps    | 95500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.8       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19099      |\n",
      "|    policy_loss        | -18.5      |\n",
      "|    reward             | -0.9908205 |\n",
      "|    reward_max         | 1.5886747  |\n",
      "|    reward_mean        | 0.41290334 |\n",
      "|    reward_min         | -0.9908205 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 7.09       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 692        |\n",
      "|    iterations         | 19200      |\n",
      "|    time_elapsed       | 138        |\n",
      "|    total_timesteps    | 96000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.83      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19199      |\n",
      "|    policy_loss        | -46.3      |\n",
      "|    reward             | -1.1380196 |\n",
      "|    reward_max         | 4.0589843  |\n",
      "|    reward_mean        | 0.49207905 |\n",
      "|    reward_min         | -1.7868418 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 43         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 692       |\n",
      "|    iterations         | 19300     |\n",
      "|    time_elapsed       | 139       |\n",
      "|    total_timesteps    | 96500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.85     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19299     |\n",
      "|    policy_loss        | -5.05     |\n",
      "|    reward             | 4.3446817 |\n",
      "|    reward_max         | 4.3446817 |\n",
      "|    reward_mean        | 1.2287028 |\n",
      "|    reward_min         | -2.49012  |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 0.763     |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 692          |\n",
      "|    iterations         | 19400        |\n",
      "|    time_elapsed       | 140          |\n",
      "|    total_timesteps    | 97000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.86        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 19399        |\n",
      "|    policy_loss        | 13.8         |\n",
      "|    reward             | -0.104816236 |\n",
      "|    reward_max         | 0.8189403    |\n",
      "|    reward_mean        | -0.2526328   |\n",
      "|    reward_min         | -1.1794044   |\n",
      "|    std                | 1.17         |\n",
      "|    value_loss         | 2.25         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 692        |\n",
      "|    iterations         | 19500      |\n",
      "|    time_elapsed       | 140        |\n",
      "|    total_timesteps    | 97500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.86      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19499      |\n",
      "|    policy_loss        | -12.2      |\n",
      "|    reward             | 4.1020665  |\n",
      "|    reward_max         | 4.287162   |\n",
      "|    reward_mean        | 2.0413928  |\n",
      "|    reward_min         | -1.9977739 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 15.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 692        |\n",
      "|    iterations         | 19600      |\n",
      "|    time_elapsed       | 141        |\n",
      "|    total_timesteps    | 98000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.86      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19599      |\n",
      "|    policy_loss        | 55.4       |\n",
      "|    reward             | 3.0687604  |\n",
      "|    reward_max         | 3.0687604  |\n",
      "|    reward_mean        | 0.2729325  |\n",
      "|    reward_min         | -1.8778769 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 45.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 693        |\n",
      "|    iterations         | 19700      |\n",
      "|    time_elapsed       | 142        |\n",
      "|    total_timesteps    | 98500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.87      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19699      |\n",
      "|    policy_loss        | -21.3      |\n",
      "|    reward             | 3.0479999  |\n",
      "|    reward_max         | 4.1165833  |\n",
      "|    reward_mean        | 0.9201061  |\n",
      "|    reward_min         | -6.4269137 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 17.2       |\n",
      "--------------------------------------\n",
      "day: 2011, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4213352.89\n",
      "total_reward: 3213352.89\n",
      "total_cost: 999.00\n",
      "total_trades: 6033\n",
      "Sharpe: 0.937\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 693         |\n",
      "|    iterations         | 19800       |\n",
      "|    time_elapsed       | 142         |\n",
      "|    total_timesteps    | 99000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.88       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19799       |\n",
      "|    policy_loss        | 0.72        |\n",
      "|    reward             | 1.6713524   |\n",
      "|    reward_max         | 1.6713524   |\n",
      "|    reward_mean        | -0.07845916 |\n",
      "|    reward_min         | -2.6899526  |\n",
      "|    std                | 1.17        |\n",
      "|    value_loss         | 0.983       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 693        |\n",
      "|    iterations         | 19900      |\n",
      "|    time_elapsed       | 143        |\n",
      "|    total_timesteps    | 99500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.9       |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19899      |\n",
      "|    policy_loss        | -10.7      |\n",
      "|    reward             | 2.8310058  |\n",
      "|    reward_max         | 4.418793   |\n",
      "|    reward_mean        | 0.2922504  |\n",
      "|    reward_min         | -6.2036066 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 8.19       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 693        |\n",
      "|    iterations         | 20000      |\n",
      "|    time_elapsed       | 144        |\n",
      "|    total_timesteps    | 100000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.89      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19999      |\n",
      "|    policy_loss        | 2.93       |\n",
      "|    reward             | 1.3868213  |\n",
      "|    reward_max         | 1.9044878  |\n",
      "|    reward_mean        | 0.48195833 |\n",
      "|    reward_min         | -1.7100815 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 1.23       |\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "agent = FinRLAgent(env_train)\n",
    "trained_a2c = agent.train_model(\"a2c\", logger_dir=\"../\" + config.RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a91a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model(\"../\" + config.MODELS_DIR, config.TEST_NAME + \"_a2c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7036f25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_a2c = agent.load_model(\n",
    "    \"a2c\", \"../\" + config.MODELS_DIR, config.TEST_NAME + \"_a2c\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c5af99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_env, (env_trade, obs_trade) = environment.get_trade_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d14aac66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "df_account, df_actions = agent.test_model(\n",
    "    gym_env,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271284e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
